<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head><meta content="text/html; charset=utf-8" http-equiv="Content-Type"/><title>8.8.4.2. sklearn.feature_extraction.text.HashingVectorizer — scikit-learn 0.13.1 documentation</title><link href="../../_static/nature.css" rel="stylesheet" type="text/css"/><link href="../../_static/pygments.css" rel="stylesheet" type="text/css"/><script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.13.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script><script src="../../_static/jquery.js" type="text/javascript"></script><script src="../../_static/underscore.js" type="text/javascript"></script><script src="../../_static/doctools.js" type="text/javascript"></script><script src="../../_static/sidebar.js" type="text/javascript"></script><link href="../../_static/favicon.ico" rel="shortcut icon"/><link href="../../about.html" rel="author" title="About these documents"/><link href="../../index.html" rel="top" title="scikit-learn 0.13.1 documentation"/><link href="../classes.html" rel="up" title="8. Reference"/><link href="sklearn.feature_extraction.text.TfidfTransformer.html" rel="next" title="8.8.4.3. sklearn.feature_extraction.text.TfidfTransformer"/><link href="sklearn.feature_extraction.text.CountVectorizer.html" rel="prev" title="8.8.4.1. sklearn.feature_extraction.text.CountVectorizer"/><script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script></head><body>
<div class="header-wrapper">
<div class="header">
<p class="logo"><a href="../../index.html">
<img alt="Logo" src="../../_static/scikit-learn-logo-small.png"/>
</a>
</p><div class="navbar">
<ul>
<li><a href="../../install.html">Download</a></li>
<li><a href="../../support.html">Support</a></li>
<li><a href="../../user_guide.html">User Guide</a></li>
<li><a href="../../auto_examples/index.html">Examples</a></li>
<li><a href="../classes.html">Reference</a></li>
</ul>
<div class="search_form">
<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>
</div>
</div> <!-- end navbar --></div>
</div>
<div class="content-wrapper">
<div class="sphinxsidebar">
<div class="sphinxsidebarwrapper">
<div class="rel">
<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
<div class="rellink">
<a accesskey="P" href="sklearn.feature_extraction.text.CountVectorizer.html" title="8.8.4.1. sklearn.feature_extraction.text.CountVectorizer">Previous
	    <br/>
<span class="smallrellink">
	    8.8.4.1. sklearn...
	    </span>
<span class="hiddenrellink">
	    8.8.4.1. sklearn.feature_extraction.text.CountVectorizer
	    </span>
</a>
</div>
<div class="spacer">
	     
	    </div>
<div class="rellink">
<a accesskey="N" href="sklearn.feature_extraction.text.TfidfTransformer.html" title="8.8.4.3. sklearn.feature_extraction.text.TfidfTransformer">Next
	    <br/>
<span class="smallrellink">
	    8.8.4.3. sklearn...
	    </span>
<span class="hiddenrellink">
	    8.8.4.3. sklearn.feature_extraction.text.TfidfTransformer
	    </span>
</a>
</div>
<div class="spacer">
	     
	    </div>
<div class="rellink">
<a href="../../np-modindex.html" title="Python Module Index">Modules
	    <br/>
<span class="smallrellink">
	    Python Module In...
	    </span>
<span class="hiddenrellink">
	    Python Module Index
	    </span>
</a>
</div>
<div class="spacer">
	     
	    </div>
<div class="rellink">
<a href="../../py-modindex.html" title="Python Module Index">Modules
	    <br/>
<span class="smallrellink">
	    Python Module In...
	    </span>
<span class="hiddenrellink">
	    Python Module Index
	    </span>
</a>
</div>
<!-- Ad a link to the 'up' page -->
<div class="spacer">
	 
	</div>
<div class="rellink">
<a href="../classes.html" title="8. Reference">
	Up
	<br/>
<span class="smallrellink">
	8. Reference
	</span>
<span class="hiddenrellink">
	8. Reference
	</span>
</a>
</div>
</div>
<p style="text-align: center; background-color: #FFE4E4">This documentation is
    for scikit-learn <strong>version 0.13.1</strong>
    — <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
<h3><a href="../../about.html#citing-scikit-learn">Citing</a></h3>
<p>If you use the software, please consider
    <a href="../../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
<h3>This page</h3>
<ul>
<li><a class="reference internal" href="#">8.8.4.2. sklearn.feature_extraction.text.HashingVectorizer</a></li>
</ul>
</div>
</div>
<div class="content">
<div class="documentwrapper">
<div class="bodywrapper">
<div class="body">
<div class="section" id="sklearn-feature-extraction-text-hashingvectorizer">
<h1>8.8.4.2. sklearn.feature_extraction.text.HashingVectorizer<a class="headerlink" href="#sklearn-feature-extraction-text-hashingvectorizer" title="Permalink to this headline">¶</a></h1>
<dl class="class">
<dt id="sklearn.feature_extraction.text.HashingVectorizer"><a name="//apple_ref/cpp/cl/sklearn.feature_extraction.text.HashingVectorizer"></a>
<em class="property">class </em><tt class="descclassname">sklearn.feature_extraction.text.</tt><tt class="descname">HashingVectorizer</tt><big>(</big><em>input='content'</em>, <em>charset='utf-8'</em>, <em>charset_error='strict'</em>, <em>strip_accents=None</em>, <em>lowercase=True</em>, <em>preprocessor=None</em>, <em>tokenizer=None</em>, <em>stop_words=None</em>, <em>token_pattern=u'(?u)\b\w\w+\b'</em>, <em>ngram_range=(1</em>, <em>1)</em>, <em>analyzer='word'</em>, <em>n_features=1048576</em>, <em>binary=False</em>, <em>norm='l2'</em>, <em>non_negative=False</em>, <em>dtype=&lt;type 'numpy.float64'&gt;</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Convert a collection of text documents to a matrix of token occurrences</p>
<p>It turns a collection of text documents into a scipy.sparse matrix holding
token occurrence counts (or binary occurrence information), possibly
normalized as token frequencies if norm=’l1’ or projected on the euclidean
unit sphere if norm=’l2’.</p>
<p>This text vectorizer implementation uses the hashing trick to find the
token string name to feature integer index mapping.</p>
<p>This strategy has several advantage:</p>
<ul class="simple">
<li>it is very low memory scalable to large datasets as there is no need to
store a vocabulary dictionary in memory</li>
<li>it is fast to pickle and un-pickle has it holds no state besides the
constructor parameters</li>
<li>it can be used in a streaming (partial fit) or parallel pipeline as there
is no state computed during fit.</li>
</ul>
<p>There are also a couple of cons (vs using a CountVectorizer with an
in-memory vocabulary):</p>
<ul class="simple">
<li>there is no way to compute the inverse transform (from feature indices to
string feature names) which can be a problem when trying to introspect
which features are most important to a model.</li>
<li>there can be collisions: distinct tokens can be mapped to the same
feature index. However in practice this is rarely an issue if n_features
is large enough (e.g. 2 ** 18 for text classification problems).</li>
<li>no IDF weighting as this would render the transformer stateful.</li>
</ul>
<p>The hash function employed is the signed 32-bit version of Murmurhash3.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>input: string {‘filename’, ‘file’, ‘content’}</strong> :</p>
<blockquote>
<div><p>If filename, the sequence passed as an argument to fit is
expected to be a list of filenames that need reading to fetch
the raw content to analyze.</p>
<p>If ‘file’, the sequence items must have ‘read’ method (file-like
object) it is called to fetch the bytes in memory.</p>
<p>Otherwise the input is expected to be the sequence strings or
bytes items are expected to be analyzed directly.</p>
</div></blockquote>
<p><strong>charset: string, ‘utf-8’ by default.</strong> :</p>
<blockquote>
<div><p>If bytes or files are given to analyze, this charset is used to
decode.</p>
</div></blockquote>
<p><strong>charset_error: {‘strict’, ‘ignore’, ‘replace’}</strong> :</p>
<blockquote>
<div><p>Instruction on what to do if a byte sequence is given to analyze that
contains characters not of the given <cite>charset</cite>. By default, it is
‘strict’, meaning that a UnicodeDecodeError will be raised. Other
values are ‘ignore’ and ‘replace’.</p>
</div></blockquote>
<p><strong>strip_accents: {‘ascii’, ‘unicode’, None}</strong> :</p>
<blockquote>
<div><p>Remove accents during the preprocessing step.
‘ascii’ is a fast method that only works on characters that have
an direct ASCII mapping.
‘unicode’ is a slightly slower method that works on any characters.
None (default) does nothing.</p>
</div></blockquote>
<p><strong>analyzer: string, {‘word’, ‘char’, ‘char_wb’} or callable</strong> :</p>
<blockquote>
<div><p>Whether the feature should be made of word or character n-grams.
Option ‘char_wb’ creates character n-grams only from text inside
word boundaries.</p>
<p>If a callable is passed it is used to extract the sequence of features
out of the raw, unprocessed input.</p>
</div></blockquote>
<p><strong>preprocessor: callable or None (default)</strong> :</p>
<blockquote>
<div><p>Override the preprocessing (string transformation) stage while
preserving the tokenizing and n-grams generation steps.</p>
</div></blockquote>
<p><strong>tokenizer: callable or None (default)</strong> :</p>
<blockquote>
<div><p>Override the string tokenization step while preserving the
preprocessing and n-grams generation steps.</p>
</div></blockquote>
<p><strong>ngram_range: tuple (min_n, max_n)</strong> :</p>
<blockquote>
<div><p>The lower and upper boundary of the range of n-values for different
n-grams to be extracted. All values of n such that min_n &lt;= n &lt;= max_n
will be used.</p>
</div></blockquote>
<p><strong>stop_words: string {‘english’}, list, or None (default)</strong> :</p>
<blockquote>
<div><p>If a string, it is passed to _check_stop_list and the appropriate stop
list is returned. ‘english’ is currently the only supported string
value.</p>
<p>If a list, that list is assumed to contain stop words, all of which
will be removed from the resulting tokens.</p>
</div></blockquote>
<p><strong>lowercase: boolean, default True</strong> :</p>
<blockquote>
<div><p>Convert all characters to lowercase before tokenizing.</p>
</div></blockquote>
<p><strong>token_pattern: string</strong> :</p>
<blockquote>
<div><p>Regular expression denoting what constitutes a “token”, only used
if <cite>tokenize == ‘word’</cite>. The default regexp select tokens of 2
or more letters characters (punctuation is completely ignored
and always treated as a token separator).</p>
</div></blockquote>
<p><strong>n_features</strong> : interger, optional, (2 ** 20) by default</p>
<blockquote>
<div><p>The number of features (columns) in the output matrices. Small numbers
of features are likely to cause hash collisions, but large numbers
will cause larger coefficient dimensions in linear learners.</p>
</div></blockquote>
<p><strong>norm</strong> : ‘l1’, ‘l2’ or None, optional</p>
<blockquote>
<div><p>Norm used to normalize term vectors. None for no normalization.</p>
</div></blockquote>
<p><strong>binary: boolean, False by default.</strong> :</p>
<blockquote>
<div><p>If True, all non zero counts are set to 1. This is useful for discrete
probabilistic models that model binary events rather than integer
counts.</p>
</div></blockquote>
<p><strong>dtype: type, optional</strong> :</p>
<blockquote>
<div><p>Type of the matrix returned by fit_transform() or transform().</p>
</div></blockquote>
<p><strong>non_negative</strong> : boolean, optional</p>
<blockquote class="last">
<div><p>Whether output matrices should contain non-negative values only;
effectively calls abs on the matrix prior to returning it.
When True, output values will be multinomially distributed.
When False, output values will be normally distributed (Gaussian) with
mean 0, assuming a good hash function.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
<div class="admonition-see-also admonition seealso">
<p class="first admonition-title">See also</p>
<p class="last"><a class="reference internal" href="sklearn.feature_extraction.text.CountVectorizer.html#sklearn.feature_extraction.text.CountVectorizer" title="sklearn.feature_extraction.text.CountVectorizer"><tt class="xref py py-obj docutils literal"><span class="pre">CountVectorizer</span></tt></a>, <a class="reference internal" href="sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer" title="sklearn.feature_extraction.text.TfidfVectorizer"><tt class="xref py py-obj docutils literal"><span class="pre">TfidfVectorizer</span></tt></a></p>
</div>
<p class="rubric">Methods</p>
<table border="1" class="longtable docutils">
<colgroup>
<col width="10%"></col>
<col width="90%"></col>
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_analyzer" title="sklearn.feature_extraction.text.HashingVectorizer.build_analyzer"><tt class="xref py py-obj docutils literal"><span class="pre">build_analyzer</span></tt></a>()</td>
<td>Return a callable that handles preprocessing and tokenization</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor" title="sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor"><tt class="xref py py-obj docutils literal"><span class="pre">build_preprocessor</span></tt></a>()</td>
<td>Return a function to preprocess the text before tokenization</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer" title="sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer"><tt class="xref py py-obj docutils literal"><span class="pre">build_tokenizer</span></tt></a>()</td>
<td>Return a function that split a string in sequence of tokens</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.decode" title="sklearn.feature_extraction.text.HashingVectorizer.decode"><tt class="xref py py-obj docutils literal"><span class="pre">decode</span></tt></a>(doc)</td>
<td>Decode the input into a string of unicode symbols</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.fit" title="sklearn.feature_extraction.text.HashingVectorizer.fit"><tt class="xref py py-obj docutils literal"><span class="pre">fit</span></tt></a>(X[, y])</td>
<td>Does nothing: this transformer is stateless.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.fit_transform" title="sklearn.feature_extraction.text.HashingVectorizer.fit_transform"><tt class="xref py py-obj docutils literal"><span class="pre">fit_transform</span></tt></a>(X[, y])</td>
<td>Transform a sequence of instances to a scipy.sparse matrix.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.get_params" title="sklearn.feature_extraction.text.HashingVectorizer.get_params"><tt class="xref py py-obj docutils literal"><span class="pre">get_params</span></tt></a>([deep])</td>
<td>Get parameters for the estimator</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.get_stop_words" title="sklearn.feature_extraction.text.HashingVectorizer.get_stop_words"><tt class="xref py py-obj docutils literal"><span class="pre">get_stop_words</span></tt></a>()</td>
<td>Build or fetch the effective stop words list</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.partial_fit" title="sklearn.feature_extraction.text.HashingVectorizer.partial_fit"><tt class="xref py py-obj docutils literal"><span class="pre">partial_fit</span></tt></a>(X[, y])</td>
<td>Does nothing: this transformer is stateless.</td>
</tr>
<tr class="row-even"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.set_params" title="sklearn.feature_extraction.text.HashingVectorizer.set_params"><tt class="xref py py-obj docutils literal"><span class="pre">set_params</span></tt></a>(**params)</td>
<td>Set the parameters of the estimator.</td>
</tr>
<tr class="row-odd"><td><a class="reference internal" href="#sklearn.feature_extraction.text.HashingVectorizer.transform" title="sklearn.feature_extraction.text.HashingVectorizer.transform"><tt class="xref py py-obj docutils literal"><span class="pre">transform</span></tt></a>(X[, y])</td>
<td>Transform a sequence of instances to a scipy.sparse matrix.</td>
</tr>
</tbody>
</table>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.__init__"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.__init__"></a>
<tt class="descname">__init__</tt><big>(</big><em>input='content'</em>, <em>charset='utf-8'</em>, <em>charset_error='strict'</em>, <em>strip_accents=None</em>, <em>lowercase=True</em>, <em>preprocessor=None</em>, <em>tokenizer=None</em>, <em>stop_words=None</em>, <em>token_pattern=u'(?u)\b\w\w+\b'</em>, <em>ngram_range=(1</em>, <em>1)</em>, <em>analyzer='word'</em>, <em>n_features=1048576</em>, <em>binary=False</em>, <em>norm='l2'</em>, <em>non_negative=False</em>, <em>dtype=&lt;type 'numpy.float64'&gt;</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.__init__" title="Permalink to this definition">¶</a></dt>
<dd></dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.build_analyzer"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.build_analyzer"></a>
<tt class="descname">build_analyzer</tt><big>(</big><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.build_analyzer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a callable that handles preprocessing and tokenization</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor"></a>
<tt class="descname">build_preprocessor</tt><big>(</big><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.build_preprocessor" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a function to preprocess the text before tokenization</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer"></a>
<tt class="descname">build_tokenizer</tt><big>(</big><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.build_tokenizer" title="Permalink to this definition">¶</a></dt>
<dd><p>Return a function that split a string in sequence of tokens</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.decode"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.decode"></a>
<tt class="descname">decode</tt><big>(</big><em>doc</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.decode" title="Permalink to this definition">¶</a></dt>
<dd><p>Decode the input into a string of unicode symbols</p>
<p>The decoding strategy depends on the vectorizer parameters.</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.fit"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.fit"></a>
<tt class="descname">fit</tt><big>(</big><em>X</em>, <em>y=None</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Does nothing: this transformer is stateless.</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.fit_transform"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.fit_transform"></a>
<tt class="descname">fit_transform</tt><big>(</big><em>X</em>, <em>y=None</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.fit_transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a sequence of instances to a scipy.sparse matrix.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>X</strong> : iterable over raw text documents, length = n_samples</p>
<blockquote>
<div><p>Samples. Each sample must be a text document (either bytes or
unicode strings, filen ame or file object depending on the
constructor argument) which will be tokenized and hashed.</p>
</div></blockquote>
<p><strong>y</strong> : (ignored)</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns :</th><td class="field-body"><p class="first"><strong>X</strong> : scipy.sparse matrix, shape = (n_samples, self.n_features)</p>
<blockquote class="last">
<div><p>Feature matrix, for use with estimators or further transformers.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.get_params"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.get_params"></a>
<tt class="descname">get_params</tt><big>(</big><em>deep=True</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.get_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Get parameters for the estimator</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>deep: boolean, optional</strong> :</p>
<blockquote class="last">
<div><p>If True, will return the parameters for this estimator and
contained subobjects that are estimators.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.get_stop_words"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.get_stop_words"></a>
<tt class="descname">get_stop_words</tt><big>(</big><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.get_stop_words" title="Permalink to this definition">¶</a></dt>
<dd><p>Build or fetch the effective stop words list</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.partial_fit"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.partial_fit"></a>
<tt class="descname">partial_fit</tt><big>(</big><em>X</em>, <em>y=None</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.partial_fit" title="Permalink to this definition">¶</a></dt>
<dd><p>Does nothing: this transformer is stateless.</p>
<p>This method is just there to mark the fact that this transformer
can work in a streaming setup.</p>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.set_params"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.set_params"></a>
<tt class="descname">set_params</tt><big>(</big><em>**params</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.set_params" title="Permalink to this definition">¶</a></dt>
<dd><p>Set the parameters of the estimator.</p>
<p>The method works on simple estimators as well as on nested objects
(such as pipelines). The former have parameters of the form
<tt class="docutils literal"><span class="pre">&lt;component&gt;__&lt;parameter&gt;</span></tt> so that it’s possible to update each
component of a nested object.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Returns :</th><td class="field-body"><strong>self</strong> :</td>
</tr>
</tbody>
</table>
</dd></dl>
<dl class="method">
<dt id="sklearn.feature_extraction.text.HashingVectorizer.transform"><a name="//apple_ref/cpp/clm/sklearn.feature_extraction.text.HashingVectorizer.transform"></a>
<tt class="descname">transform</tt><big>(</big><em>X</em>, <em>y=None</em><big>)</big><a class="headerlink" href="#sklearn.feature_extraction.text.HashingVectorizer.transform" title="Permalink to this definition">¶</a></dt>
<dd><p>Transform a sequence of instances to a scipy.sparse matrix.</p>
<table class="docutils field-list" frame="void" rules="none">
<col class="field-name"></col>
<col class="field-body"></col>
<tbody valign="top">
<tr class="field-odd field"><th class="field-name">Parameters :</th><td class="field-body"><p class="first"><strong>X</strong> : iterable over raw text documents, length = n_samples</p>
<blockquote>
<div><p>Samples. Each sample must be a text document (either bytes or
unicode strings, filen ame or file object depending on the
constructor argument) which will be tokenized and hashed.</p>
</div></blockquote>
<p><strong>y</strong> : (ignored)</p>
</td>
</tr>
<tr class="field-even field"><th class="field-name">Returns :</th><td class="field-body"><p class="first"><strong>X</strong> : scipy.sparse matrix, shape = (n_samples, self.n_features)</p>
<blockquote class="last">
<div><p>Feature matrix, for use with estimators or further transformers.</p>
</div></blockquote>
</td>
</tr>
</tbody>
</table>
</dd></dl>
</dd></dl>
</div>
</div>
</div>
</div>
<div class="clearer"></div>
</div>
</div>
<div class="footer">
        © 2010–2013, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3. Design by <a href="http://desgrana.es">Desgrana</a>.
    <span style="padding-left: 5ex;">
<a href="../../_sources/modules/generated/sklearn.feature_extraction.text.HashingVectorizer.txt" rel="nofollow">Show this page source</a>
</span>
</div>
<div class="rel">
<div class="buttonPrevious">
<a href="sklearn.feature_extraction.text.CountVectorizer.html">
        Previous
      </a>
</div>
<div class="buttonNext">
<a href="sklearn.feature_extraction.text.TfidfTransformer.html">
        Next
      </a>
</div>
<div class="buttonPrevious">
<a href="../../np-modindex.html">
        Previous
      </a>
</div>
<div class="buttonNext">
<a href="../../py-modindex.html">
        Next
      </a>
</div>
</div>
<script type="text/javascript">
       $("div.buttonNext, div.buttonPrevious").hover(
           function () {
               $(this).css('background-color', '#FF9C34');
           },
           function () {
               $(this).css('background-color', '#A7D6E2');
           }
       );
       var bodywrapper = $('.bodywrapper');
   	var sidebarbutton = $('#sidebarbutton');
        sidebarbutton.css({
	    'height': '900px'
       });
     </script>
</body></html>