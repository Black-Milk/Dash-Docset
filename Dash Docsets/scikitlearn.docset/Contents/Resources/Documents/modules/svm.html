

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3.2. Support Vector Machines &mdash; scikit-learn 0.13.1 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.13.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/sidebar.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.13.1 documentation" href="../index.html" />
    <link rel="up" title="3. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="3.3. Stochastic Gradient Descent" href="sgd.html" />
    <link rel="prev" title="3.1. Generalized Linear Models" href="linear_model.html" />


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>



  </head>
  <body>

    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

      <div class="sphinxsidebar">
	<div class="sphinxsidebarwrapper">
	  <div class="rel">
	   
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="linear_model.html" title="3.1. Generalized Linear Models"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    3.1. Generalized...
	    </span>
	    <span class="hiddenrellink">
	    3.1. Generalized Linear Models
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="sgd.html" title="3.3. Stochastic Gradient Descent"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    3.3. Stochastic ...
	    </span>
	    <span class="hiddenrellink">
	    3.3. Stochastic Gradient Descent
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="../np-modindex.html" title="Python Module Index"
	    >Modules
	    <br>
	    <span class="smallrellink">
	    Python Module In...
	    </span>
	    <span class="hiddenrellink">
	    Python Module Index
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="../py-modindex.html" title="Python Module Index"
	    >Modules
	    <br>
	    <span class="smallrellink">
	    Python Module In...
	    </span>
	    <span class="hiddenrellink">
	    Python Module Index
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="../supervised_learning.html" title="3. Supervised learning" >
	Up
	<br>
	<span class="smallrellink">
	3. Supervised le...
	</span>
	<span class="hiddenrellink">
	3. Supervised learning
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center; background-color: #FFE4E4">This documentation is
    for scikit-learn <strong>version 0.13.1</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3><a href="../about.html#citing-scikit-learn">Citing</a></h3>
    <p>If you use the software, please consider
    <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">3.2. Support Vector Machines</a><ul>
<li><a class="reference internal" href="#classification">3.2.1. Classification</a><ul>
<li><a class="reference internal" href="#multi-class-classification">3.2.1.1. Multi-class classification</a></li>
<li><a class="reference internal" href="#scores-and-probabilities">3.2.1.2. Scores and probabilities</a></li>
<li><a class="reference internal" href="#unbalanced-problems">3.2.1.3. Unbalanced problems</a></li>
</ul>
</li>
<li><a class="reference internal" href="#regression">3.2.2. Regression</a></li>
<li><a class="reference internal" href="#density-estimation-novelty-detection">3.2.3. Density estimation, novelty detection</a></li>
<li><a class="reference internal" href="#complexity">3.2.4. Complexity</a></li>
<li><a class="reference internal" href="#tips-on-practical-use">3.2.5. Tips on Practical Use</a></li>
<li><a class="reference internal" href="#kernel-functions">3.2.6. Kernel functions</a><ul>
<li><a class="reference internal" href="#custom-kernels">3.2.6.1. Custom Kernels</a><ul>
<li><a class="reference internal" href="#using-python-functions-as-kernels">3.2.6.1.1. Using Python functions as kernels</a></li>
<li><a class="reference internal" href="#using-the-gram-matrix">3.2.6.1.2. Using the Gram matrix</a></li>
<li><a class="reference internal" href="#parameters-of-the-rbf-kernel">3.2.6.1.3. Parameters of the RBF Kernel</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#mathematical-formulation">3.2.7. Mathematical formulation</a><ul>
<li><a class="reference internal" href="#svc">3.2.7.1. SVC</a></li>
<li><a class="reference internal" href="#nusvc">3.2.7.2. NuSVC</a></li>
</ul>
</li>
<li><a class="reference internal" href="#implementation-details">3.2.8. Implementation details</a></li>
</ul>
</li>
</ul>

    
    </div>
	  </div>


      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="support-vector-machines">
<span id="svm"></span><h1>3.2. Support Vector Machines<a class="headerlink" href="#support-vector-machines" title="Permalink to this headline">Â¶</a></h1>
<p><strong>Support vector machines (SVMs)</strong> are a set of supervised learning
methods used for <a class="reference internal" href="#svm-classification"><em>classification</em></a>,
<a class="reference internal" href="#svm-regression"><em>regression</em></a> and <a class="reference internal" href="#svm-outlier-detection"><em>outliers detection</em></a>.</p>
<p>The advantages of support vector machines are:</p>
<blockquote>
<div><ul class="simple">
<li>Effective in high dimensional spaces.</li>
<li>Still effective in cases where number of dimensions is greater
than the number of samples.</li>
<li>Uses a subset of training points in the decision function (called
support vectors), so it is also memory efficient.</li>
<li>Versatile: different <a class="reference internal" href="#svm-kernels"><em>Kernel functions</em></a> can be
specified for the decision function. Common kernels are
provided, but it is also possible to specify custom kernels.</li>
</ul>
</div></blockquote>
<p>The disadvantages of support vector machines include:</p>
<blockquote>
<div><ul class="simple">
<li>If the number of features is much greater than the number of
samples, the method is likely to give poor performances.</li>
<li>SVMs do not directly provide probability estimates, these are
calculated using an expensive five-fold cross-validation
(see <em class="xref std std-ref">Scores and probabilities</em>, below).</li>
</ul>
</div></blockquote>
<p>The support vector machines in scikit-learn support both dens
(<tt class="docutils literal"><span class="pre">numpy.ndarray</span></tt> and convertible to that by <tt class="docutils literal"><span class="pre">numpy.asarray</span></tt>) and
sparse (any <tt class="docutils literal"><span class="pre">scipy.sparse</span></tt>) sample vectors as input. However, to use
an SVM to make predictions for sparse data, it must have been fit on such
data. For optimal performance, use C-ordered <tt class="docutils literal"><span class="pre">numpy.ndarray</span></tt> (dense) or
<tt class="docutils literal"><span class="pre">scipy.sparse.csr_matrix</span></tt> (sparse) with <tt class="docutils literal"><span class="pre">dtype=float64</span></tt>.</p>
<div class="section" id="classification">
<span id="svm-classification"></span><h2>3.2.1. Classification<a class="headerlink" href="#classification" title="Permalink to this headline">Â¶</a></h2>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> are classes
capable of performing multi-class classification on a dataset.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_iris.html"><img alt="../_images/plot_iris_12.png" src="../_images/plot_iris_12.png" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> are similar methods, but accept
slightly different sets of parameters and have different mathematical
formulations (see section <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a>). On the
other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> is another implementation of Support
Vector Classification for the case of a linear kernel. Note that
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> does not accept keyword <tt class="docutils literal"><span class="pre">kernel</span></tt>, as this is
assumed to be linear. It also lacks some of the members of
<a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, like <tt class="docutils literal"><span class="pre">support_</span></tt>.</p>
<p>As other classifiers, <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> take as input two arrays: an array X of size <tt class="docutils literal"><span class="pre">[n_samples,</span>
<span class="pre">n_features]</span></tt> holding the training samples, and an array Y of integer values,
size <tt class="docutils literal"><span class="pre">[n_samples]</span></tt>, holding the class labels for the training samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=0.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, shrinking=True,</span>
<span class="go">tol=0.001, verbose=False)</span>
</pre></div>
</div>
<p>After being fitted, the model can then be used to predict new values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="go">array([1])</span>
</pre></div>
</div>
<p>SVMs decision function depends on some subset of the training data,
called the support vectors. Some properties of these support vectors
can be found in members <tt class="docutils literal"><span class="pre">support_vectors_</span></tt>, <tt class="docutils literal"><span class="pre">support_</span></tt> and
<tt class="docutils literal"><span class="pre">n_support</span></tt>:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="c"># get support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_vectors_</span>
<span class="go">array([[ 0.,  0.],</span>
<span class="go">       [ 1.,  1.]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get indices of support vectors</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">support_</span> 
<span class="go">array([0, 1]...)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># get number of support vectors for each class</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">n_support_</span> 
<span class="go">array([1, 1]...)</span>
</pre></div>
</div>
<div class="section" id="multi-class-classification">
<span id="svm-multi-class"></span><h3>3.2.1.1. Multi-class classification<a class="headerlink" href="#multi-class-classification" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> and <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> implement the &#8220;one-against-one&#8221;
approach (Knerr et al., 1990) for multi- class classification. If
<tt class="docutils literal"><span class="pre">n_class</span></tt> is the number of classes, then <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></tt>
classifiers are constructed and each one trains data from two classes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">],</span> <span class="p">[</span><span class="mi">3</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">Y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=0.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False, shrinking=True,</span>
<span class="go">tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="c"># 4 classes: 4*3/2 = 6</span>
<span class="go">6</span>
</pre></div>
</div>
<p>On the other hand, <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implements &#8220;one-vs-the-rest&#8221;
multi-class strategy, thus training n_class models. If there are only
two classes, only one model is trained:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">LinearSVC</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lin_clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">Y</span><span class="p">)</span> 
<span class="go">LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,</span>
<span class="go">intercept_scaling=1, loss=&#39;l2&#39;, multi_class=&#39;ovr&#39;, penalty=&#39;l2&#39;,</span>
<span class="go">random_state=None, tol=0.0001, verbose=0)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span> <span class="o">=</span> <span class="n">lin_clf</span><span class="o">.</span><span class="n">decision_function</span><span class="p">([[</span><span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">dec</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
<span class="go">4</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="#svm-mathematical-formulation"><em>Mathematical formulation</em></a> for a complete description of
the decision function.</p>
<p>Note that the <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> also implements an alternative multi-class
strategy, the so-called multi-class SVM formulated by Crammer and Singer, by
using the option <tt class="docutils literal"><span class="pre">multi_class='crammer_singer'</span></tt>. This method is consistent,
which is not true for one-vs-rest classification.
In practice, on-vs-rest classification is usually preferred, since the results
are mostly similar, but the runtime is significantly less.</p>
<p>For &#8220;one-vs-rest&#8221; <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> the attributes <tt class="docutils literal"><span class="pre">coef_</span></tt> and <tt class="docutils literal"><span class="pre">intercept_</span></tt>
have the shape <tt class="docutils literal"><span class="pre">[n_class,</span> <span class="pre">n_features]</span></tt> and <tt class="docutils literal"><span class="pre">[n_class]</span></tt> respectively.
Each row of the coefficients corresponds to one of the <tt class="docutils literal"><span class="pre">n_class</span></tt> many
&#8220;one-vs-rest&#8221; classifiers and simliar for the interecepts, in the
order of the &#8220;one&#8221; class.</p>
<p>In the case of &#8220;one-vs-one&#8221; <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, the layout of the attributes
is a little more involved. In the case of having a linear kernel,
The layout of <tt class="docutils literal"><span class="pre">coef_</span></tt> and <tt class="docutils literal"><span class="pre">intercept_</span></tt> is similar to the one
described for <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> described above, except that the shape of
<tt class="docutils literal"><span class="pre">coef_</span></tt> is <tt class="docutils literal"><span class="pre">[n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2,</span> <span class="pre">n_features]</span></tt>, corresponding to as
many binary classifiers. The order for classes
0 to n is &#8220;0 vs 1&#8221;, &#8220;0 vs 2&#8221; , ... &#8220;0 vs n&#8221;, &#8220;1 vs 2&#8221;, &#8220;1 vs 3&#8221;, &#8220;1 vs n&#8221;, . .
. &#8220;n-1 vs n&#8221;.</p>
<p>The shape of <tt class="docutils literal"><span class="pre">dual_coef_</span></tt> is <tt class="docutils literal"><span class="pre">[n_class-1,</span> <span class="pre">n_SV]</span></tt> with
a somewhat hard to grasp layout.
The columns correspond to the support vectors involved in any
of the <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">*</span> <span class="pre">(n_class</span> <span class="pre">-</span> <span class="pre">1)</span> <span class="pre">/</span> <span class="pre">2</span></tt> &#8220;one-vs-one&#8221; classifiers.
Each of the support vectors is used in <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></tt> classifiers.
The <tt class="docutils literal"><span class="pre">n_class</span> <span class="pre">-</span> <span class="pre">1</span></tt> entries in each row correspond to the dual coefficients
for these classifiers.</p>
<p>This might be made more clear by an example:</p>
<p>Consider a three class problem with with class 0 having three support vectors
<img class="math" src="../_images/math/421f60d3b5560d5dfa098fda140a1671e216c65f.png" alt="v^{0}_0, v^{1}_0, v^{2}_0"/> and class 1 and 2 having two support vectors
<img class="math" src="../_images/math/5e560f2b1854f96e2d455311e2593d5429ffa78e.png" alt="v^{0}_1, v^{1}_1"/> and <img class="math" src="../_images/math/5e560f2b1854f96e2d455311e2593d5429ffa78e.png" alt="v^{0}_1, v^{1}_1"/> respectively.  For each
support vector <img class="math" src="../_images/math/c988973fead8b3040b3985509be86938fea6e7fa.png" alt="v^{j}_i"/>, there are two dual coefficients.  Let&#8217;s call
the coefficient of support vector <img class="math" src="../_images/math/c988973fead8b3040b3985509be86938fea6e7fa.png" alt="v^{j}_i"/> in the classifier between
classes <cite>i</cite> and <cite>k</cite> <img class="math" src="../_images/math/8b03b125c6b3c91df87c02c7e7858426bd928c48.png" alt="\alpha^{j}_{i,k}"/>.  Then <tt class="docutils literal"><span class="pre">dual_coef_</span></tt> looks like
this:</p>
<table border="1" class="docutils">
<colgroup>
<col width="36%" />
<col width="36%" />
<col width="27%" />
</colgroup>
<tbody valign="top">
<tr class="row-odd"><td><img class="math" src="../_images/math/da127577a7daa5d7543fcaeabfa911a3babe7ed0.png" alt="\alpha^{0}_{0,1}"/></td>
<td><img class="math" src="../_images/math/42a9af7401c61b1641b6f941968ce24ec7595a13.png" alt="\alpha^{0}_{0,2}"/></td>
<td rowspan="3">Coefficients
for SVs of class 0</td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/fe0ef400d241f9cd7166d12066debb749204c269.png" alt="\alpha^{1}_{0,1}"/></td>
<td><img class="math" src="../_images/math/36488fb45eb71c1fc2dc7188deef64367e4790b3.png" alt="\alpha^{1}_{0,2}"/></td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/1e4957af83ff0fc3313bf3a729d98cc13a26515e.png" alt="\alpha^{2}_{0,1}"/></td>
<td><img class="math" src="../_images/math/54e705927657eba7942fa5d78efcec4918ff960c.png" alt="\alpha^{2}_{0,2}"/></td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/d622b5bb4062b87b1aa043e83f739d1d508a6fdf.png" alt="\alpha^{0}_{1,0}"/></td>
<td><img class="math" src="../_images/math/1e70845f14f8393032f489280c42092132c732f0.png" alt="\alpha^{0}_{1,2}"/></td>
<td rowspan="2">Coefficients
for SVs of class 1</td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/c2a5a030027fb6cd12f62fe1c6f8f769de9a7f0a.png" alt="\alpha^{1}_{1,0}"/></td>
<td><img class="math" src="../_images/math/f269c8766a84c395bdede1b12f352079bd232366.png" alt="\alpha^{1}_{1,2}"/></td>
</tr>
<tr class="row-even"><td><img class="math" src="../_images/math/8413ee6f5048ca08c30c56562fe4d6330f18d64a.png" alt="\alpha^{0}_{2,0}"/></td>
<td><img class="math" src="../_images/math/d07311fe219a4465936ba186768e3f71c7ae48b0.png" alt="\alpha^{0}_{2,1}"/></td>
<td rowspan="2">Coefficients
for SVs of class 2</td>
</tr>
<tr class="row-odd"><td><img class="math" src="../_images/math/5e92d7962655b23823c06bae96cb373d8dd47680.png" alt="\alpha^{1}_{2,0}"/></td>
<td><img class="math" src="../_images/math/22e8c1d955977e15d24672bbd38f619151eff837.png" alt="\alpha^{1}_{2,1}"/></td>
</tr>
</tbody>
</table>
</div>
<div class="section" id="scores-and-probabilities">
<span id="scores-probabilities"></span><h3>3.2.1.2. Scores and probabilities<a class="headerlink" href="#scores-and-probabilities" title="Permalink to this headline">Â¶</a></h3>
<p>The <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> method <tt class="docutils literal"><span class="pre">decision_function</span></tt> gives per-class scores
for each sample (or a single score per sample in the binary case).
When the constructor option <tt class="docutils literal"><span class="pre">probability</span></tt> is set to <tt class="docutils literal"><span class="pre">True</span></tt>,
class membership probability estimates
(from the methods <tt class="docutils literal"><span class="pre">predict_proba</span></tt> and <tt class="docutils literal"><span class="pre">predict_log_proba</span></tt>) are enabled.
In the binary case, the probabilities are calibrated using Platt scaling:
logistic regression on the SVM&#8217;s scores,
fit by an additional cross-validation on the training data.
In the multiclass case, this is extended as per Wu et al. (2004).</p>
<p>Needless to say, the cross-validation involved in Platt scaling
is an expensive operation for large datasets.
In addition, the probability estimates may be inconsistent with the scores,
in the sense that the &#8220;argmax&#8221; of the scores
may not be the argmax of the probabilities.
(E.g., in binary classification,
a sample may be labeled by <tt class="docutils literal"><span class="pre">predict</span></tt> as belonging to a class
that has probability &lt;Â½ according to <tt class="docutils literal"><span class="pre">predict_proba</span></tt>.)
Platt&#8217;s method is also known to have theoretical issues.
If confidence scores are required, but these do not have to be probabilities,
then it is advisable to set <tt class="docutils literal"><span class="pre">probability=False</span></tt>
and use <tt class="docutils literal"><span class="pre">decision_function</span></tt> instead of <tt class="docutils literal"><span class="pre">predict_proba</span></tt>.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li>Wu, Lin and Weng,
<a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/svmprob/svmprob.pdf">&#8220;Probability estimates for multi-class classification by pairwise coupling&#8221;</a>.
JMLR 5:975-1005, 2004.</li>
</ul>
</div>
</div>
<div class="section" id="unbalanced-problems">
<h3>3.2.1.3. Unbalanced problems<a class="headerlink" href="#unbalanced-problems" title="Permalink to this headline">Â¶</a></h3>
<p>In problems where it is desired to give more importance to certain
classes or certain individual samples keywords <tt class="docutils literal"><span class="pre">class_weight</span></tt> and
<tt class="docutils literal"><span class="pre">sample_weight</span></tt> can be used.</p>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> (but not <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>) implement a keyword
<tt class="docutils literal"><span class="pre">class_weight</span></tt> in the <tt class="docutils literal"><span class="pre">fit</span></tt> method. It&#8217;s a dictionary of the form
<tt class="docutils literal"><span class="pre">{class_label</span> <span class="pre">:</span> <span class="pre">value}</span></tt>, where value is a floating point number &gt; 0
that sets the parameter <tt class="docutils literal"><span class="pre">C</span></tt> of class <tt class="docutils literal"><span class="pre">class_label</span></tt> to <tt class="docutils literal"><span class="pre">C</span> <span class="pre">*</span> <span class="pre">value</span></tt>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html"><img alt="../_images/plot_separating_hyperplane_unbalanced_11.png" src="../_images/plot_separating_hyperplane_unbalanced_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<p><a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a> implement also weights for individual samples in method
<tt class="docutils literal"><span class="pre">fit</span></tt> through keyword <tt class="docutils literal"><span class="pre">sample_weight</span></tt>.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_weighted_samples.html"><img alt="../_images/plot_weighted_samples_11.png" src="../_images/plot_weighted_samples_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_iris.html#example-svm-plot-iris-py"><em>Plot different SVM classifiers in the iris dataset</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane.html#example-svm-plot-separating-hyperplane-py"><em>SVM: Maximum margin separating hyperplane</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_separating_hyperplane_unbalanced.html#example-svm-plot-separating-hyperplane-unbalanced-py"><em>SVM: Separating hyperplane for unbalanced classes</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_anova.html#example-svm-plot-svm-anova-py"><em>SVM-Anova: SVM with univariate feature selection</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_nonlinear.html#example-svm-plot-svm-nonlinear-py"><em>Non-linear SVM</em></a></li>
<li><a class="reference internal" href="../auto_examples/svm/plot_weighted_samples.html#example-svm-plot-weighted-samples-py"><em>SVM: Weighted samples</em></a>,</li>
</ul>
</div>
</div>
</div>
<div class="section" id="regression">
<span id="svm-regression"></span><h2>3.2.2. Regression<a class="headerlink" href="#regression" title="Permalink to this headline">Â¶</a></h2>
<p>The method of Support Vector Classification can be extended to solve
regression problems. This method is called Support Vector Regression.</p>
<p>The model produced by support vector classification (as described
above) depends only on a subset of the training data, because the cost
function for building the model does not care about training points
that lie beyond the margin. Analogously, the model produced by Support
Vector Regression depends only on a subset of the training data,
because the cost function for building the model ignores any training
data close to the model prediction.</p>
<p>There are two flavors of Support Vector Regression: <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>.</p>
<p>As with classification classes, the fit method will take as
argument vectors X, y, only that in this case y is expected to have
floating point values instead of integer values:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="p">[[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">2.5</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVR</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVR(C=1.0, cache_size=200, coef0=0.0, degree=3,</span>
<span class="go">epsilon=0.1, gamma=0.0, kernel=&#39;rbf&#39;, max_iter=-1, probability=False,</span>
<span class="go">shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="go">array([ 1.5])</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_svm_regression.html#example-svm-plot-svm-regression-py"><em>Support Vector Regression (SVR) using linear and non-linear kernels</em></a></li>
</ul>
</div>
</div>
<div class="section" id="density-estimation-novelty-detection">
<span id="svm-outlier-detection"></span><h2>3.2.3. Density estimation, novelty detection<a class="headerlink" href="#density-estimation-novelty-detection" title="Permalink to this headline">Â¶</a></h2>
<p>One-class SVM is used for novelty detection, that is, given a set of
samples, it will detect the soft boundary of that set so as to
classify new points as belonging to that set or not. The class that
implements this is called <a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a>.</p>
<p>In this case, as it is a type of unsupervised learning, the fit method
will only take as input an array X, as there are no class labels.</p>
<p>See, section <a class="reference internal" href="outlier_detection.html#outlier-detection"><em>Novelty and Outlier Detection</em></a> for more details on this usage.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/svm/plot_oneclass.html"><img alt="../_images/plot_oneclass_11.png" src="../_images/plot_oneclass_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_oneclass.html#example-svm-plot-oneclass-py"><em>One-class SVM with non-linear kernel (RBF)</em></a></li>
<li><a class="reference internal" href="../auto_examples/applications/plot_species_distribution_modeling.html#example-applications-plot-species-distribution-modeling-py"><em>Species distribution modeling</em></a></li>
</ul>
</div>
</div>
<div class="section" id="complexity">
<h2>3.2.4. Complexity<a class="headerlink" href="#complexity" title="Permalink to this headline">Â¶</a></h2>
<p>Support Vector Machines are powerful tools, but their compute and
storage requirements increase rapidly with the number of training
vectors. The core of an SVM is a quadratic programming problem (QP),
separating support vectors from the rest of the training data. The QP
solver used by this <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based implementation scales between
<img class="math" src="../_images/math/1f1311c79b186e9a1e4aabcf5892dffc661c0264.png" alt="O(n_{features} \times n_{samples}^2)"/> and
<img class="math" src="../_images/math/7fa7e2716acbd8ca70826fdacdbb446261ee7f51.png" alt="O(n_{features} \times n_{samples}^3)"/> depending on how efficiently
the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> cache is used in practice (dataset dependent). If the data
is very sparse <img class="math" src="../_images/math/02e494853b2b09e9c00e1c6b80864bc8b016bf5a.png" alt="n_{features}"/> should be replaced by the average number
of non-zero features in a sample vector.</p>
<p>Also note that for the linear case, the algorithm used in
<a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> by the <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> implementation is much more
efficient than its <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a>-based <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a> counterpart and can
scale almost linearly to millions of samples and/or features.</p>
</div>
<div class="section" id="tips-on-practical-use">
<h2>3.2.5. Tips on Practical Use<a class="headerlink" href="#tips-on-practical-use" title="Permalink to this headline">Â¶</a></h2>
<blockquote>
<div><ul>
<li><p class="first"><strong>Avoiding data copy</strong>: For <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a> and
<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>, if the data passed to certain methods is not C-ordered
contiguous, and double precision, it will be copied before calling the
underlying C implementation. You can check whether a give numpy array is
C-contiguous by inspecting its <cite>flags</cite> attribute.</p>
<p>For <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> (and <a class="reference internal" href="generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression" title="sklearn.linear_model.LogisticRegression"><tt class="xref py py-class docutils literal"><span class="pre">LogisticRegression</span></tt></a>) any input passed as a numpy
array will be copied and converted to the liblinear internal sparse data
representation (double precision floats and int32 indices of non-zero
components). If you want to fit a large-scale linear classifier without
copying a dense numpy C-contiguous double precision array as input we
suggest to use the <a class="reference internal" href="generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier" title="sklearn.linear_model.SGDClassifier"><tt class="xref py py-class docutils literal"><span class="pre">SGDClassifier</span></tt></a> class instead.  The objective
function can be configured to be almost the same as the <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a>
model.</p>
</li>
<li><p class="first"><strong>Kernel cache size</strong>: For <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, <a class="reference internal" href="generated/sklearn.svm.SVR.html#sklearn.svm.SVR" title="sklearn.svm.SVR"><tt class="xref py py-class docutils literal"><span class="pre">SVR</span></tt></a>, <tt class="xref py py-class docutils literal"><span class="pre">nuSVC</span></tt> and
<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>, the size of the kernel cache has a strong impact on run
times for larger problems.  If you have enough RAM available, it is
recommended to set <tt class="docutils literal"><span class="pre">cache_size</span></tt> to a higher value than the default of
200(MB), such as 500(MB) or 1000(MB).</p>
</li>
<li><p class="first"><strong>Setting C</strong>: <tt class="docutils literal"><span class="pre">C</span></tt> is <tt class="docutils literal"><span class="pre">1</span></tt> by default and it&#8217;s a reasonable default
choice.  If you have a lot of noisy observations you should decrease it.
It corresponds to regularize more the estimation.</p>
</li>
<li><p class="first">Support Vector Machine algorithms are not scale invariant, so <strong>it
is highly recommended to scale your data</strong>. For example, scale each
attribute on the input vector X to [0,1] or [-1,+1], or standardize it
to have mean 0 and variance 1. Note that the <em>same</em> scaling must be
applied to the test vector to obtain meaningful results. See section
<a class="reference internal" href="preprocessing.html#preprocessing"><em>Preprocessing data</em></a> for more details on scaling and normalization.</p>
</li>
<li><p class="first">Parameter <tt class="docutils literal"><span class="pre">nu</span></tt> in <a class="reference internal" href="generated/sklearn.svm.NuSVC.html#sklearn.svm.NuSVC" title="sklearn.svm.NuSVC"><tt class="xref py py-class docutils literal"><span class="pre">NuSVC</span></tt></a>/<a class="reference internal" href="generated/sklearn.svm.OneClassSVM.html#sklearn.svm.OneClassSVM" title="sklearn.svm.OneClassSVM"><tt class="xref py py-class docutils literal"><span class="pre">OneClassSVM</span></tt></a>/<a class="reference internal" href="generated/sklearn.svm.NuSVR.html#sklearn.svm.NuSVR" title="sklearn.svm.NuSVR"><tt class="xref py py-class docutils literal"><span class="pre">NuSVR</span></tt></a>
approximates the fraction of training errors and support vectors.</p>
</li>
<li><p class="first">In <a class="reference internal" href="generated/sklearn.svm.SVC.html#sklearn.svm.SVC" title="sklearn.svm.SVC"><tt class="xref py py-class docutils literal"><span class="pre">SVC</span></tt></a>, if data for classification are unbalanced (e.g. many
positive and few negative), set <tt class="docutils literal"><span class="pre">class_weight='auto'</span></tt> and/or try
different penalty parameters <tt class="docutils literal"><span class="pre">C</span></tt>.</p>
</li>
<li><p class="first">The underlying <a class="reference internal" href="generated/sklearn.svm.LinearSVC.html#sklearn.svm.LinearSVC" title="sklearn.svm.LinearSVC"><tt class="xref py py-class docutils literal"><span class="pre">LinearSVC</span></tt></a> implementation uses a random
number generator to select features when fitting the model. It is
thus not uncommon, to have slightly different results for the same
input data. If that happens, try with a smaller tol parameter.</p>
</li>
<li><p class="first">Using L1 penalization as provided by <tt class="docutils literal"><span class="pre">LinearSVC(loss='l2',</span> <span class="pre">penalty='l1',</span>
<span class="pre">dual=False)</span></tt> yields a sparse solution, i.e. only a subset of feature
weights is different from zero and contribute to the decision function.
Increasing <tt class="docutils literal"><span class="pre">C</span></tt> yields a more complex model (more feature are selected).
The <tt class="docutils literal"><span class="pre">C</span></tt> value that yields a &#8220;null&#8221; model (all weights equal to zero) can
be calculated using <a class="reference internal" href="generated/sklearn.svm.l1_min_c.html#sklearn.svm.l1_min_c" title="sklearn.svm.l1_min_c"><tt class="xref py py-func docutils literal"><span class="pre">l1_min_c</span></tt></a>.</p>
</li>
</ul>
</div></blockquote>
</div>
<div class="section" id="kernel-functions">
<span id="svm-kernels"></span><h2>3.2.6. Kernel functions<a class="headerlink" href="#kernel-functions" title="Permalink to this headline">Â¶</a></h2>
<p>The <em>kernel function</em> can be any of the following:</p>
<blockquote>
<div><ul class="simple">
<li>linear: <img class="math" src="../_images/math/335466a1d9412ae1cbeb16e982842a63548d5c88.png" alt="\langle x, x'\rangle"/>.</li>
<li>polynomial: <img class="math" src="../_images/math/32c664ae249c848cd8455b326acb9b937d882b1b.png" alt="(\gamma \langle x, x'\rangle + r)^d"/>. <cite>d</cite> is specified by
keyword <tt class="docutils literal"><span class="pre">degree</span></tt>, <cite>r</cite> by <tt class="docutils literal"><span class="pre">coef0</span></tt>.</li>
<li>rbf: <img class="math" src="../_images/math/190818eacf35a0c45f76932d77ac2a440e855a24.png" alt="\exp(-\gamma |x-x'|^2)"/>. <img class="math" src="../_images/math/66981fa3920210c6ad8dbe5e968783d5dd7520c3.png" alt="\gamma"/> is
specified by keyword <tt class="docutils literal"><span class="pre">gamma</span></tt>, must be greater than 0.</li>
<li>sigmoid (<img class="math" src="../_images/math/4829b35686dacea41811753be471cf71ccbfb8df.png" alt="\tanh(\gamma \langle x,x'\rangle + r)"/>), where <cite>r</cite> is specified by
<tt class="docutils literal"><span class="pre">coef0</span></tt>.</li>
</ul>
</div></blockquote>
<p>Different kernels are specified by keyword kernel at initialization:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">linear_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;linear&#39;</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;rbf&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">rbf_svc</span><span class="o">.</span><span class="n">kernel</span>
<span class="go">&#39;rbf&#39;</span>
</pre></div>
</div>
<div class="section" id="custom-kernels">
<h3>3.2.6.1. Custom Kernels<a class="headerlink" href="#custom-kernels" title="Permalink to this headline">Â¶</a></h3>
<p>You can define your own kernels by either giving the kernel as a
python function or by precomputing the Gram matrix.</p>
<p>Classifiers with custom kernels behave the same way as any other
classifiers, except that:</p>
<blockquote>
<div><ul class="simple">
<li>Field <tt class="docutils literal"><span class="pre">support_vectors_</span></tt> is now empty, only indices of support
vectors are stored in <tt class="docutils literal"><span class="pre">support_</span></tt></li>
<li>A reference (and not a copy) of the first argument in the <tt class="docutils literal"><span class="pre">fit()</span></tt>
method is stored for future reference. If that array changes between the
use of <tt class="docutils literal"><span class="pre">fit()</span></tt> and <tt class="docutils literal"><span class="pre">predict()</span></tt> you will have unexpected results.</li>
</ul>
</div></blockquote>
<div class="section" id="using-python-functions-as-kernels">
<h4>3.2.6.1.1. Using Python functions as kernels<a class="headerlink" href="#using-python-functions-as-kernels" title="Permalink to this headline">Â¶</a></h4>
<p>You can also use your own defined kernels by passing a function to the
keyword <tt class="docutils literal"><span class="pre">kernel</span></tt> in the constructor.</p>
<p>Your kernel must take as arguments two matrices and return a third matrix.</p>
<p>The following code defines a linear kernel and creates a classifier
instance that will use that kernel:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">my_kernel</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="n">my_kernel</span><span class="p">)</span>
</pre></div>
</div>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_custom_kernel.html#example-svm-plot-custom-kernel-py"><em>SVM with custom kernel</em></a>.</li>
</ul>
</div>
</div>
<div class="section" id="using-the-gram-matrix">
<h4>3.2.6.1.2. Using the Gram matrix<a class="headerlink" href="#using-the-gram-matrix" title="Permalink to this headline">Â¶</a></h4>
<p>Set <tt class="docutils literal"><span class="pre">kernel='precomputed'</span></tt> and pass the Gram matrix instead of X in the fit
method. At the moment, the kernel values between <cite>all</cite> training vectors and the
test vectors must be provided.</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;precomputed&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># linear kernel computation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gram</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">T</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">gram</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> 
<span class="go">SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0, degree=3,</span>
<span class="go">gamma=0.0, kernel=&#39;precomputed&#39;, max_iter=-1, probability=False,</span>
<span class="go">shrinking=True, tol=0.001, verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="c"># predict on training examples</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">gram</span><span class="p">)</span>
<span class="go">array([0, 1])</span>
</pre></div>
</div>
</div>
<div class="section" id="parameters-of-the-rbf-kernel">
<h4>3.2.6.1.3. Parameters of the RBF Kernel<a class="headerlink" href="#parameters-of-the-rbf-kernel" title="Permalink to this headline">Â¶</a></h4>
<p>When training an SVM with the <em>Radial Basis Function</em> (RBF) kernel, two
parameters must be considered: <tt class="docutils literal"><span class="pre">C</span></tt> and <tt class="docutils literal"><span class="pre">gamma</span></tt>.  The parameter <tt class="docutils literal"><span class="pre">C</span></tt>,
common to all SVM kernels, trades off misclassification of training examples
against simplicity of the decision surface. A low <tt class="docutils literal"><span class="pre">C</span></tt> makes the decision
surface smooth, while a high <tt class="docutils literal"><span class="pre">C</span></tt> aims at classifying all training examples
correctly.  <tt class="docutils literal"><span class="pre">gamma</span></tt> defines how much influence a single training example has.
The larger <tt class="docutils literal"><span class="pre">gamma</span></tt> is, the closer other examples must be to be affected.</p>
<p>Proper choice of <tt class="docutils literal"><span class="pre">C</span></tt> and <tt class="docutils literal"><span class="pre">gamma</span></tt> is critical to the SVM&#8217;s performance.  One
is advised to use <tt class="xref py py-class docutils literal"><span class="pre">GridSearchCV</span></tt> with <tt class="docutils literal"><span class="pre">C</span></tt> and <tt class="docutils literal"><span class="pre">gamma</span></tt> spaced
exponentially far apart to choose good values.</p>
<div class="topic">
<p class="topic-title first">Examples:</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/svm/plot_rbf_parameters.html#example-svm-plot-rbf-parameters-py"><em>RBF SVM parameters</em></a></li>
</ul>
</div>
</div>
</div>
</div>
<div class="section" id="mathematical-formulation">
<span id="svm-mathematical-formulation"></span><h2>3.2.7. Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">Â¶</a></h2>
<p>A support vector machine constructs a hyper-plane or set of hyper-planes
in a high or infinite dimensional space, which can be used for
classification, regression or other tasks. Intuitively, a good
separation is achieved by the hyper-plane that has the largest distance
to the nearest training data points of any class (so-called functional
margin), since in general the larger the margin the lower the
generalization error of the classifier.</p>
<div class="figure align-center">
<a class="reference internal image-reference" href="../_images/plot_separating_hyperplane_11.png"><img alt="../_images/plot_separating_hyperplane_11.png" src="../_images/plot_separating_hyperplane_11.png" style="width: 600.0px; height: 450.0px;" /></a>
</div>
<div class="section" id="svc">
<h3>3.2.7.1. SVC<a class="headerlink" href="#svc" title="Permalink to this headline">Â¶</a></h3>
<p>Given training vectors <img class="math" src="../_images/math/a09ea178decd47aa7b07b21c6d97ef5347e2e602.png" alt="x_i \in R^p"/>, i=1,..., n, in two classes, and a
vector <img class="math" src="../_images/math/fc785dee08aa3f880f748fd19d636092154dd5f6.png" alt="y \in R^n"/> such that <img class="math" src="../_images/math/bd856b5b3c2391f4708d88e138140aea21a62e3a.png" alt="y_i \in \{1, -1\}"/>, SVC solves the
following primal problem:</p>
<div class="math">
<p><img src="../_images/math/b45838fb8b981db8fe3bf693bc4fac7fef62eb94.png" alt="\min_ {w, b, \zeta} \frac{1}{2} w^T w + C \sum_{i=1, n} \zeta_i



\textrm {subject to } &amp; y_i (w^T \phi (x_i) + b) \geq 1 - \zeta_i,\\
&amp; \zeta_i \geq 0, i=1, ..., n"/></p>
</div><p>Its dual is</p>
<div class="math">
<p><img src="../_images/math/2e17355bcd769a4b9d93acb2eccb9c3c4151a3c1.png" alt="\min_{\alpha} \frac{1}{2} \alpha^T Q \alpha - e^T \alpha


\textrm {subject to } &amp; y^T \alpha = 0\\
&amp; 0 \leq \alpha_i \leq C, i=1, ..., l"/></p>
</div><p>where <img class="math" src="../_images/math/a3a59bb1293ee3f6dec19de4019a7178874219ae.png" alt="e"/> is the vector of all ones, <img class="math" src="../_images/math/15fe7c24b95b0390a222eb87117ed76c54619e78.png" alt="C &gt; 0"/> is the upper bound,
<img class="math" src="../_images/math/9866e3a998d628ba0941eb4fea0666ac391d149a.png" alt="Q"/> is an <cite>n</cite> by <cite>n</cite> positive semidefinite matrix, <img class="math" src="../_images/math/18d5298274473c734193290b05a2ef0c5512db29.png" alt="Q_{ij} \equiv
K(x_i, x_j)"/> and <img class="math" src="../_images/math/2519dc33443f6b4bb0fd9cdc24ad7ba4e662ddea.png" alt="\phi (x_i)^T \phi (x)"/> is the kernel. Here training
vectors are mapped into a higher (maybe infinite) dimensional space by the
function <img class="math" src="../_images/math/2c175f60eecef1de7560c3bdea495d69f26f719d.png" alt="\phi"/>.</p>
<p>The decision function is:</p>
<div class="math">
<p><img src="../_images/math/b804ba19c8a25a49b59f61aae4ae7401ac958d60.png" alt="\operatorname{sgn}(\sum_{i=1}^n y_i \alpha_i K(x_i, x) + \rho)"/></p>
</div><div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">While SVM models derived from <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> and <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> use <tt class="docutils literal"><span class="pre">C</span></tt> as
regularization parameter, most other estimators use <tt class="docutils literal"><span class="pre">alpha</span></tt>. The relation
between both is <img class="math" src="../_images/math/60cfe108dd35b5586db50a3ec6b25d9e8fc32154.png" alt="C = \frac{n\_samples}{alpha}"/>.</p>
</div>
<p>This parameters can be accessed through the members <cite>dual_coef_</cite>
which holds the product <img class="math" src="../_images/math/91e0ab578768e5a3fdb5657751c33d9f167ef639.png" alt="y_i \alpha_i"/>, <cite>support_vectors_</cite> which
holds the support vectors, and <cite>intercept_</cite> which holds the independent
term <img class="math" src="../_images/math/55fa4f9455edd1c68ca06e7969c9c58d075ac409.png" alt="-\rho"/> :</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.17.7215">&#8220;Automatic Capacity Tuning of Very Large VC-dimension Classifiers&#8221;</a>
I Guyon, B Boser, V Vapnik - Advances in neural information
processing 1993,</li>
<li><a class="reference external" href="http://www.springerlink.com/content/k238jx04hm87j80g/">&#8220;Support-vector networks&#8221;</a>
C. Cortes, V. Vapnik, Machine Leaming, 20, 273-297 (1995)</li>
</ul>
</div>
</div>
<div class="section" id="nusvc">
<h3>3.2.7.2. NuSVC<a class="headerlink" href="#nusvc" title="Permalink to this headline">Â¶</a></h3>
<p>We introduce a new parameter <img class="math" src="../_images/math/d6a7ccf879c4a4fe694033606332cb83806db296.png" alt="\nu"/> which controls the number of
support vectors and training errors. The parameter <img class="math" src="../_images/math/948360fd417f20a48ae449aa26ede1baef981854.png" alt="\nu \in (0,
1]"/> is an upper bound on the fraction of training errors and a lower
bound of the fraction of support vectors.</p>
<p>It can be shown that the <cite>nu</cite>-SVC formulation is a reparametrization
of the <cite>C</cite>-SVC and therefore mathematically equivalent.</p>
</div>
</div>
<div class="section" id="implementation-details">
<h2>3.2.8. Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">Â¶</a></h2>
<p>Internally, we use <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/libsvm/">libsvm</a> and <a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">liblinear</a> to handle all
computations. These libraries are wrapped using C and Cython.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<p>For a description of the implementation and details of the algorithms
used, please refer to</p>
<blockquote>
<div><ul class="simple">
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/papers/libsvm.pdf">LIBSVM: a library for Support Vector Machines</a></li>
<li><a class="reference external" href="http://www.csie.ntu.edu.tw/~cjlin/liblinear/">LIBLINEAR &#8211; A Library for Large Linear Classification</a></li>
</ul>
</div></blockquote>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010â2013, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3. Design by <a href="http://desgrana.es">Desgrana</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/svm.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="linear_model.html">
        Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="sgd.html">
        Next
      </a>
    </div>
    <div class="buttonPrevious">
      <a href="../np-modindex.html">
        Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="../py-modindex.html">
        Next
      </a>
    </div>
    
     </div>
     <script type="text/javascript">
       $("div.buttonNext, div.buttonPrevious").hover(
           function () {
               $(this).css('background-color', '#FF9C34');
           },
           function () {
               $(this).css('background-color', '#A7D6E2');
           }
       );
       var bodywrapper = $('.bodywrapper');
   	var sidebarbutton = $('#sidebarbutton');
        sidebarbutton.css({
	    'height': '900px'
       });
     </script>
  </body>
</html>