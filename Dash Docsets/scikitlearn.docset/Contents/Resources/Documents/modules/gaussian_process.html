

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>3.5. Gaussian Processes &mdash; scikit-learn 0.13.1 documentation</title>
    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.13.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/sidebar.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.13.1 documentation" href="../index.html" />
    <link rel="up" title="3. Supervised learning" href="../supervised_learning.html" />
    <link rel="next" title="3.6. Partial Least Squares" href="pls.html" />
    <link rel="prev" title="3.4. Nearest Neighbors" href="neighbors.html" />


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>



  </head>
  <body>

    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../install.html">Download</a></li>
            <li><a href="../support.html">Support</a></li>
            <li><a href="../user_guide.html">User Guide</a></li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            <li><a href="classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

      <div class="sphinxsidebar">
	<div class="sphinxsidebarwrapper">
	  <div class="rel">
	   
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="neighbors.html" title="3.4. Nearest Neighbors"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    3.4. Nearest Nei...
	    </span>
	    <span class="hiddenrellink">
	    3.4. Nearest Neighbors
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="pls.html" title="3.6. Partial Least Squares"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    3.6. Partial Lea...
	    </span>
	    <span class="hiddenrellink">
	    3.6. Partial Least Squares
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="../np-modindex.html" title="Python Module Index"
	    >Modules
	    <br>
	    <span class="smallrellink">
	    Python Module In...
	    </span>
	    <span class="hiddenrellink">
	    Python Module Index
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="../py-modindex.html" title="Python Module Index"
	    >Modules
	    <br>
	    <span class="smallrellink">
	    Python Module In...
	    </span>
	    <span class="hiddenrellink">
	    Python Module Index
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="../supervised_learning.html" title="3. Supervised learning" >
	Up
	<br>
	<span class="smallrellink">
	3. Supervised le...
	</span>
	<span class="hiddenrellink">
	3. Supervised learning
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center; background-color: #FFE4E4">This documentation is
    for scikit-learn <strong>version 0.13.1</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3><a href="../about.html#citing-scikit-learn">Citing</a></h3>
    <p>If you use the software, please consider
    <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">3.5. Gaussian Processes</a><ul>
<li><a class="reference internal" href="#examples">3.5.1. Examples</a><ul>
<li><a class="reference internal" href="#an-introductory-regression-example">3.5.1.1. An introductory regression example</a></li>
<li><a class="reference internal" href="#fitting-noisy-data">3.5.1.2. Fitting Noisy Data</a></li>
</ul>
</li>
<li><a class="reference internal" href="#mathematical-formulation">3.5.2. Mathematical formulation</a><ul>
<li><a class="reference internal" href="#the-initial-assumption">3.5.2.1. The initial assumption</a></li>
<li><a class="reference internal" href="#the-best-linear-unbiased-prediction-blup">3.5.2.2. The best linear unbiased prediction (BLUP)</a></li>
<li><a class="reference internal" href="#the-empirical-best-linear-unbiased-predictor-eblup">3.5.2.3. The empirical best linear unbiased predictor (EBLUP)</a></li>
</ul>
</li>
<li><a class="reference internal" href="#correlation-models">3.5.3. Correlation Models</a></li>
<li><a class="reference internal" href="#regression-models">3.5.4. Regression Models</a></li>
<li><a class="reference internal" href="#implementation-details">3.5.5. Implementation details</a></li>
</ul>
</li>
</ul>

    
    </div>
	  </div>


      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="gaussian-processes">
<span id="gaussian-process"></span><h1>3.5. Gaussian Processes<a class="headerlink" href="#gaussian-processes" title="Permalink to this headline">¶</a></h1>
<p><strong>Gaussian Processes for Machine Learning (GPML)</strong> is a generic supervised
learning method primarily designed to solve <em>regression</em> problems. It has also
been extended to <em>probabilistic classification</em>, but in the present
implementation, this is only a post-processing of the <em>regression</em> exercise.</p>
<p>The advantages of Gaussian Processes for Machine Learning are:</p>
<blockquote>
<div><ul class="simple">
<li>The prediction interpolates the observations (at least for regular
correlation models).</li>
<li>The prediction is probabilistic (Gaussian) so that one can compute
empirical confidence intervals and exceedence probabilities that might be
used to refit (online fitting, adaptive fitting) the prediction in some
region of interest.</li>
<li>Versatile: different <a class="reference internal" href="linear_model.html#linear-model"><em>linear regression models</em></a> and <a class="reference internal" href="#correlation-models"><em>correlation models</em></a> can be specified. Common models are provided, but
it is also possible to specify custom models provided they are
stationary.</li>
</ul>
</div></blockquote>
<p>The disadvantages of Gaussian Processes for Machine Learning include:</p>
<blockquote>
<div><ul class="simple">
<li>It is not sparse. It uses the whole samples/features information to
perform the prediction.</li>
<li>It loses efficiency in high dimensional spaces &#8211; namely when the number
of features exceeds a few dozens. It might indeed give poor performance
and it loses computational efficiency.</li>
<li>Classification is only a post-processing, meaning that one first need
to solve a regression problem by providing the complete scalar float
precision output <img class="math" src="../_images/math/092e364e1d9d19ad5fffb0b46ef4cc7f2da02c1c.png" alt="y"/> of the experiment one attempt to model.</li>
</ul>
</div></blockquote>
<p>Thanks to the Gaussian property of the prediction, it has been given varied
applications: e.g. for global optimization, probabilistic classification.</p>
<div class="section" id="examples">
<h2>3.5.1. Examples<a class="headerlink" href="#examples" title="Permalink to this headline">¶</a></h2>
<div class="section" id="an-introductory-regression-example">
<h3>3.5.1.1. An introductory regression example<a class="headerlink" href="#an-introductory-regression-example" title="Permalink to this headline">¶</a></h3>
<p>Say we want to surrogate the function <img class="math" src="../_images/math/9e666d25f2804bce8d8f2e5f7dc57ba3561457c7.png" alt="g(x) = x \sin(x)"/>. To do so,
the function is evaluated onto a design of experiments. Then, we define a
GaussianProcess model whose regression and correlation models might be
specified using additional kwargs, and ask for the model to be fitted to the
data. Depending on the number of parameters provided at instantiation, the
fitting procedure may recourse to maximum likelihood estimation for the
parameters or alternatively it uses the given parameters.</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/gaussian_process/plot_gp_regression.html"><img alt="../_images/plot_gp_regression_11.png" src="../_images/plot_gp_regression_11.png" /></a>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">gaussian_process</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">f</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
<span class="gp">... </span>    <span class="k">return</span> <span class="n">x</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">([</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">3.</span><span class="p">,</span> <span class="mf">5.</span><span class="p">,</span> <span class="mf">6.</span><span class="p">,</span> <span class="mf">7.</span><span class="p">,</span> <span class="mf">8.</span><span class="p">])</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">f</span><span class="p">(</span><span class="n">X</span><span class="p">)</span><span class="o">.</span><span class="n">ravel</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="mi">1000</span><span class="p">))</span><span class="o">.</span><span class="n">T</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gp</span> <span class="o">=</span> <span class="n">gaussian_process</span><span class="o">.</span><span class="n">GaussianProcess</span><span class="p">(</span><span class="n">theta0</span><span class="o">=</span><span class="mf">1e-2</span><span class="p">,</span> <span class="n">thetaL</span><span class="o">=</span><span class="mf">1e-4</span><span class="p">,</span> <span class="n">thetaU</span><span class="o">=</span><span class="mf">1e-1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">gp</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>  
<span class="go">GaussianProcess(beta0=None, corr=&lt;function squared_exponential at 0x...&gt;,</span>
<span class="go">        normalize=True, nugget=array(2.22...-15),</span>
<span class="go">        optimizer=&#39;fmin_cobyla&#39;, random_start=1, random_state=...</span>
<span class="go">        regr=&lt;function constant at 0x...&gt;, storage_mode=&#39;full&#39;,</span>
<span class="go">        theta0=array([[ 0.01]]), thetaL=array([[ 0.0001]]),</span>
<span class="go">        thetaU=array([[ 0.1]]), verbose=False)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y_pred</span><span class="p">,</span> <span class="n">sigma2_pred</span> <span class="o">=</span> <span class="n">gp</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">eval_MSE</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="section" id="fitting-noisy-data">
<h3>3.5.1.2. Fitting Noisy Data<a class="headerlink" href="#fitting-noisy-data" title="Permalink to this headline">¶</a></h3>
<p>When the data to be fit includes noise, the Gaussian process model can be
used by specifying the variance of the noise for each point.
<a class="reference internal" href="generated/sklearn.gaussian_process.GaussianProcess.html#sklearn.gaussian_process.GaussianProcess" title="sklearn.gaussian_process.GaussianProcess"><tt class="xref py py-class docutils literal"><span class="pre">GaussianProcess</span></tt></a> takes a parameter <tt class="docutils literal"><span class="pre">nugget</span></tt> which
is added to the diagonal of the correlation matrix between training points:
in general this is a type of Tikhonov regularization.  In the special case
of a squared-exponential correlation function, this normalization is
equivalent to specifying a fractional variance in the input.  That is</p>
<div class="math">
<p><img src="../_images/math/d86eb248abab6a0308eb6d723ef6d1a1ae089dac.png" alt="\mathrm{nugget}_i = \left[\frac{\sigma_i}{y_i}\right]^2"/></p>
</div><p>With <tt class="docutils literal"><span class="pre">nugget</span></tt> and <tt class="docutils literal"><span class="pre">corr</span></tt> properly set, Gaussian Processes can be
used to robustly recover an underlying function from noisy data:</p>
<div class="figure align-center">
<a class="reference external image-reference" href="../auto_examples/gaussian_process/plot_gp_regression.html"><img alt="../_images/plot_gp_regression_21.png" src="../_images/plot_gp_regression_21.png" /></a>
</div>
<div class="topic">
<p class="topic-title first">Other examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/gaussian_process/plot_gp_probabilistic_classification_after_regression.html#example-gaussian-process-plot-gp-probabilistic-classification-after-regression-py"><em>Gaussian Processes classification example: exploiting the probabilistic output</em></a></li>
</ul>
</div>
</div>
</div>
<div class="section" id="mathematical-formulation">
<h2>3.5.2. Mathematical formulation<a class="headerlink" href="#mathematical-formulation" title="Permalink to this headline">¶</a></h2>
<div class="section" id="the-initial-assumption">
<h3>3.5.2.1. The initial assumption<a class="headerlink" href="#the-initial-assumption" title="Permalink to this headline">¶</a></h3>
<p>Suppose one wants to model the output of a computer experiment, say a
mathematical function:</p>
<div class="math">
<p><img src="../_images/math/4b9ee65fb128105280ed491cc5373d776ab0194d.png" alt="g: &amp; \mathbb{R}^{n_{\rm features}} \rightarrow \mathbb{R} \\
   &amp; X \mapsto y = g(X)"/></p>
</div><p>GPML starts with the assumption that this function is <em>a</em> conditional sample
path of <em>a</em> Gaussian process <img class="math" src="../_images/math/6e28ce12d49d39f160d5a0ef54077fc98e4b9d2b.png" alt="G"/> which is additionally assumed to read as
follows:</p>
<div class="math">
<p><img src="../_images/math/3e538ead295b88c84d7572540b8ea1f341a4061d.png" alt="G(X) = f(X)^T \beta + Z(X)"/></p>
</div><p>where <img class="math" src="../_images/math/d4f40edf15e59e0e75e9838c7050d8307fce7614.png" alt="f(X)^T \beta"/> is a linear regression model and <img class="math" src="../_images/math/4375c367ddc2ea1b22351ee8bf1c914b61d31af0.png" alt="Z(X)"/> is a
zero-mean Gaussian process with a fully stationary covariance function:</p>
<div class="math">
<p><img src="../_images/math/6d910fcba839ed0f9a3bbd1e538793a111f002cf.png" alt="C(X, X') = \sigma^2 R(|X - X'|)"/></p>
</div><p><img class="math" src="../_images/math/741fb9098efcb98055f467f87630a5d0ca599b6b.png" alt="\sigma^2"/> being its variance and <img class="math" src="../_images/math/eff43e84f8a3bcf7b6965f0a3248bc4d3a9d0cd4.png" alt="R"/> being the correlation
function which solely depends on the absolute relative distance between each
sample, possibly featurewise (this is the stationarity assumption).</p>
<p>From this basic formulation, note that GPML is nothing but an extension of a
basic least squares linear regression problem:</p>
<div class="math">
<p><img src="../_images/math/d37fbfadc00a794aafe90474a96883848c2750fb.png" alt="g(X) \approx f(X)^T \beta"/></p>
</div><p>Except we additionally assume some spatial coherence (correlation) between the
samples dictated by the correlation function. Indeed, ordinary least squares
assumes the correlation model <img class="math" src="../_images/math/5db56541a2e383e7f633b766e5f56371b58ad16e.png" alt="R(|X - X'|)"/> is one when <img class="math" src="../_images/math/e8ed95543f2a11944067f52062f98fd4114c1430.png" alt="X = X'"/>
and zero otherwise : a <em>dirac</em> correlation model &#8211; sometimes referred to as a
<em>nugget</em> correlation model in the kriging literature.</p>
</div>
<div class="section" id="the-best-linear-unbiased-prediction-blup">
<h3>3.5.2.2. The best linear unbiased prediction (BLUP)<a class="headerlink" href="#the-best-linear-unbiased-prediction-blup" title="Permalink to this headline">¶</a></h3>
<p>We now derive the <em>best linear unbiased prediction</em> of the sample path
<img class="math" src="../_images/math/311cabda3a9b09f0dde217303ca9d1cd9201dcf6.png" alt="g"/> conditioned on the observations:</p>
<div class="math">
<p><img src="../_images/math/14da2de6ab04b3c938d53e5519f825cffdfe8255.png" alt="\hat{G}(X) = G(X | y_1 = g(X_1), ...,
                            y_{n_{\rm samples}} = g(X_{n_{\rm samples}}))"/></p>
</div><p>It is derived from its <em>given properties</em>:</p>
<ul class="simple">
<li>It is linear (a linear combination of the observations)</li>
</ul>
<div class="math">
<p><img src="../_images/math/b9003fd0cec4267bec6915f974426014d1f49653.png" alt="\hat{G}(X) \equiv a(X)^T y"/></p>
</div><ul class="simple">
<li>It is unbiased</li>
</ul>
<div class="math">
<p><img src="../_images/math/dff1218e3b1f40f2b22dc06928dd50c8c81e2139.png" alt="\mathbb{E}[G(X) - \hat{G}(X)] = 0"/></p>
</div><ul class="simple">
<li>It is the best (in the Mean Squared Error sense)</li>
</ul>
<div class="math">
<p><img src="../_images/math/e91d7e7bb0f7a039856f60f86b8de0b68d544eff.png" alt="\hat{G}(X)^* = \arg \min\limits_{\hat{G}(X)} \;
                                        \mathbb{E}[(G(X) - \hat{G}(X))^2]"/></p>
</div><p>So that the optimal weight vector <img class="math" src="../_images/math/5be8339bd279277e4c26c0456fcc434e5adc60ff.png" alt="a(X)"/> is solution of the following
equality constrained optimization problem:</p>
<div class="math">
<p><img src="../_images/math/ebca6b896d5aaca6202459db146b05d1ef78f45e.png" alt="a(X)^* = \arg \min\limits_{a(X)} &amp; \; \mathbb{E}[(G(X) - a(X)^T y)^2] \\
                   {\rm s. t.} &amp; \; \mathbb{E}[G(X) - a(X)^T y] = 0"/></p>
</div><p>Rewriting this constrained optimization problem in the form of a Lagrangian and
looking further for the first order optimality conditions to be satisfied, one
ends up with a closed form expression for the sought predictor &#8211; see
references for the complete proof.</p>
<p>In the end, the BLUP is shown to be a Gaussian random variate with mean:</p>
<div class="math">
<p><img src="../_images/math/67101e358a78eb29b6e2bfe170dc3e691c0e4a0e.png" alt="\mu_{\hat{Y}}(X) = f(X)^T\,\hat{\beta} + r(X)^T\,\gamma"/></p>
</div><p>and variance:</p>
<div class="math">
<p><img src="../_images/math/0f4095dc94e5426d68143434d4008cd4235210d0.png" alt="\sigma_{\hat{Y}}^2(X) = \sigma_{Y}^2\,
( 1
- r(X)^T\,R^{-1}\,r(X)
+ u(X)^T\,(F^T\,R^{-1}\,F)^{-1}\,u(X)
)"/></p>
</div><p>where we have introduced:</p>
<ul class="simple">
<li>the correlation matrix whose terms are defined wrt the autocorrelation
function and its built-in parameters <img class="math" src="../_images/math/52e8ed7a3ba22130ad3984eb2cd413406475a689.png" alt="\theta"/>:</li>
</ul>
<div class="math">
<p><img src="../_images/math/0d31e6c5ebf07a873219bfbe04a6a21e796deb04.png" alt="R_{i\,j} = R(|X_i - X_j|, \theta), \; i,\,j = 1, ..., m"/></p>
</div><ul class="simple">
<li>the vector of cross-correlations between the point where the prediction is
made and the points in the DOE:</li>
</ul>
<div class="math">
<p><img src="../_images/math/dddfa55e5f46a5bd90d8abb02e9b977d793c050d.png" alt="r_i = R(|X - X_i|, \theta), \; i = 1, ..., m"/></p>
</div><ul class="simple">
<li>the regression matrix (eg the Vandermonde matrix if <img class="math" src="../_images/math/bb2c93730dbb48558bb3c4738c956c4e8f816437.png" alt="f"/> is a polynomial
basis):</li>
</ul>
<div class="math">
<p><img src="../_images/math/c9f8923afb17baeaf9831ee0de93608f34c2acba.png" alt="F_{i\,j} = f_i(X_j), \; i = 1, ..., p, \, j = 1, ..., m"/></p>
</div><ul class="simple">
<li>the generalized least square regression weights:</li>
</ul>
<div class="math">
<p><img src="../_images/math/cd960891d68dd68a26f2be210c2804b00a251a85.png" alt="\hat{\beta} =(F^T\,R^{-1}\,F)^{-1}\,F^T\,R^{-1}\,Y"/></p>
</div><ul class="simple">
<li>and the vectors:</li>
</ul>
<div class="math">
<p><img src="../_images/math/0bc3127c17665866ddc7f4499fa25f0bce6932f5.png" alt="\gamma &amp; = R^{-1}(Y - F\,\hat{\beta}) \\
u(X) &amp; = F^T\,R^{-1}\,r(X) - f(X)"/></p>
</div><p>It is important to notice that the probabilistic response of a Gaussian Process
predictor is fully analytic and mostly relies on basic linear algebra
operations. More precisely the mean prediction is the sum of two simple linear
combinations (dot products), and the variance requires two matrix inversions,
but the correlation matrix can be decomposed only once using a Cholesky
decomposition algorithm.</p>
</div>
<div class="section" id="the-empirical-best-linear-unbiased-predictor-eblup">
<h3>3.5.2.3. The empirical best linear unbiased predictor (EBLUP)<a class="headerlink" href="#the-empirical-best-linear-unbiased-predictor-eblup" title="Permalink to this headline">¶</a></h3>
<p>Until now, both the autocorrelation and regression models were assumed given.
In practice however they are never known in advance so that one has to make
(motivated) empirical choices for these models <a class="reference internal" href="#correlation-models"><em>Correlation Models</em></a>.</p>
<p>Provided these choices are made, one should estimate the remaining unknown
parameters involved in the BLUP. To do so, one uses the set of provided
observations in conjunction with some inference technique. The present
implementation, which is based on the DACE&#8217;s Matlab toolbox uses the <em>maximum
likelihood estimation</em> technique &#8211; see DACE manual in references for the
complete equations. This maximum likelihood estimation problem is turned into
a global optimization problem onto the autocorrelation parameters. In the
present implementation, this global optimization is solved by means of the
fmin_cobyla optimization function from scipy.optimize. In the case of
anisotropy however, we provide an implementation of Welch&#8217;s componentwise
optimization algorithm &#8211; see references.</p>
<p>For a more comprehensive description of the theoretical aspects of Gaussian
Processes for Machine Learning, please refer to the references below:</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://www2.imm.dtu.dk/~hbn/dace/">DACE, A Matlab Kriging Toolbox</a> S Lophaven, HB Nielsen, J
Sondergaard 2002</li>
<li><a class="reference external" href="http://www.jstor.org/pss/1269548">Screening, predicting, and computer experiments</a> WJ Welch, RJ Buck, J Sacks,
HP Wynn, TJ Mitchell, and MD Morris Technometrics 34(1) 15&#8211;25,
1992</li>
<li><a class="reference external" href="http://www.gaussianprocess.org/gpml/chapters/RW.pdf">Gaussian Processes for Machine Learning</a> CE
Rasmussen, CKI Williams MIT Press, 2006 (Ed. T Diettrich)</li>
<li><a class="reference external" href="http://www.stat.osu.edu/~comp_exp/book.html">The design and analysis of computer experiments</a> TJ Santner, BJ
Williams, W Notz Springer, 2003</li>
</ul>
</div>
</div>
</div>
<div class="section" id="correlation-models">
<span id="id1"></span><h2>3.5.3. Correlation Models<a class="headerlink" href="#correlation-models" title="Permalink to this headline">¶</a></h2>
<p>Common correlation models matches some famous SVM&#8217;s kernels because they are
mostly built on equivalent assumptions. They must fulfill Mercer&#8217;s conditions
and should additionally remain stationary. Note however, that the choice of the
correlation model should be made in agreement with the known properties of the
original experiment from which the observations come. For instance:</p>
<ul class="simple">
<li>If the original experiment is known to be infinitely differentiable (smooth),
then one should use the <em>squared-exponential correlation model</em>.</li>
<li>If it&#8217;s not, then one should rather use the <em>exponential correlation model</em>.</li>
<li>Note also that there exists a correlation model that takes the degree of
derivability as input: this is the Matern correlation model, but it&#8217;s not
implemented here (TODO).</li>
</ul>
<p>For a more detailed discussion on the selection of appropriate correlation
models, see the book by Rasmussen &amp; Williams in references.</p>
</div>
<div class="section" id="regression-models">
<span id="id2"></span><h2>3.5.4. Regression Models<a class="headerlink" href="#regression-models" title="Permalink to this headline">¶</a></h2>
<p>Common linear regression models involve zero- (constant), first- and
second-order polynomials. But one may specify its own in the form of a Python
function that takes the features X as input and that returns a vector
containing the values of the functional set. The only constraint is that the
number of functions must not exceed the number of available observations so
that the underlying regression problem is not <em>underdetermined</em>.</p>
</div>
<div class="section" id="implementation-details">
<h2>3.5.5. Implementation details<a class="headerlink" href="#implementation-details" title="Permalink to this headline">¶</a></h2>
<p>The present implementation is based on a translation of the DACE Matlab
toolbox.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://www2.imm.dtu.dk/~hbn/dace/">DACE, A Matlab Kriging Toolbox</a> S Lophaven, HB Nielsen, J
Sondergaard 2002,</li>
<li>W.J. Welch, R.J. Buck, J. Sacks, H.P. Wynn, T.J. Mitchell, and M.D.
Morris (1992). Screening, predicting, and computer experiments.
Technometrics, 34(1) 15&#8211;25.</li>
</ul>
</div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010–2013, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3. Design by <a href="http://desgrana.es">Desgrana</a>.
    <span style="padding-left: 5ex;">
    <a href="../_sources/modules/gaussian_process.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="neighbors.html">
        Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="pls.html">
        Next
      </a>
    </div>
    <div class="buttonPrevious">
      <a href="../np-modindex.html">
        Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="../py-modindex.html">
        Next
      </a>
    </div>
    
     </div>
     <script type="text/javascript">
       $("div.buttonNext, div.buttonPrevious").hover(
           function () {
               $(this).css('background-color', '#FF9C34');
           },
           function () {
               $(this).css('background-color', '#A7D6E2');
           }
       );
       var bodywrapper = $('.bodywrapper');
   	var sidebarbutton = $('#sidebarbutton');
        sidebarbutton.css({
	    'height': '900px'
       });
     </script>
  </body>
</html>