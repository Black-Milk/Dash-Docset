
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
  
    <title>3.1. Cross-validation: evaluating estimator performance &mdash; scikit-learn 0.14.1 documentation</title>
  <!-- htmltitle is before nature.css - we use this hack to load bootstrap first -->
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <link rel="stylesheet" href="../_static/css/bootstrap.min.css" media="screen" />
  <link rel="stylesheet" href="../_static/css/bootstrap-responsive.css"/>

    
    <link rel="stylesheet" href="../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../',
        VERSION:     '0.14.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../_static/jquery.js"></script>
    <script type="text/javascript" src="../_static/underscore.js"></script>
    <script type="text/javascript" src="../_static/doctools.js"></script>
    <script type="text/javascript" src="../_static/js/copybutton.js"></script>
    <link rel="shortcut icon" href="../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="top" title="scikit-learn 0.14.1 documentation" href="../index.html" />
    <link rel="up" title="3. Model selection and evaluation" href="../model_selection.html" />
    <link rel="next" title="3.2. Grid Search: Searching for estimator parameters" href="grid_search.html" />
    <link rel="prev" title="3. Model selection and evaluation" href="../model_selection.html" />
  
   
       <script type="text/javascript" src="../_static/sidebar.js"></script>
   
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <script src="../_static/js/bootstrap.min.js" type="text/javascript"></script>
  <link rel="canonical" href="http://scikit-learn.org/stable/modules/cross_validation.html" />

  <script type="text/javascript">
    $("div.buttonNext, div.buttonPrevious").hover(
       function () {
           $(this).css('background-color', '#FF9C34');
       },
       function () {
           $(this).css('background-color', '#A7D6E2');
       }
    );
    var bodywrapper = $('.bodywrapper');
    var sidebarbutton = $('#sidebarbutton');
    sidebarbutton.css({'height': '900px'});
  </script>

  </head>
  <body>

<div class="header-wrapper">
    <div class="header">
        <p class="logo"><a href="../index.html">
            <img src="../_static/scikit-learn-logo-small.png" alt="Logo"/>
        </a>
        </p><div class="navbar">
            <ul>
                <li><a href="../../stable/index.html">Home</a></li>
                <li><a href="../../stable/install.html">Installation</a></li>
                <li class="btn-li"><div class="btn-group">
		      <a href="../documentation.html">Documentation</a>
		      <a class="btn dropdown-toggle" data-toggle="dropdown">
			     <span class="caret"></span>
		      </a>
		      <ul class="dropdown-menu">
			<li class="link-title">Scikit-learn 0.14 (stable)</li>
			<li><a href="../tutorial/index.html">Tutorials</a></li>
			<li><a href="../user_guide.html">User guide</a></li>
			<li><a href="classes.html">API</a></li>
			<li class="divider"></li>
		        <li><a href="http://scikit-learn.org/dev/documentation.html">Development</a></li>
		        <li><a href="http://scikit-learn.org/0.13/">Scikit-learn 0.13</a></li>
		        <li><a href="http://scikit-learn.org/0.12/">Scikit-learn 0.12</a></li>
		        <li><a href="http://scikit-learn.org/0.11/">Scikit-learn 0.11</a></li>
		        <li><a href="../documentation.html">More versions...</a></li>
		      </ul>
		    </div>
		</li>
            <li><a href="../auto_examples/index.html">Examples</a></li>
            </ul>

            <div class="search_form">
                <div id="cse" style="width: 100%;"></div>
            </div>
        </div> <!-- end navbar --></div>
</div>


<!-- Github "fork me" ribbon -->
<a href="https://github.com/scikit-learn/scikit-learn">
  <img class="fork-me"
       style="position: absolute; top: 0; right: 0; border: 0;"
       src="../_static/img/forkme.png"
       alt="Fork me on GitHub" />
</a>

<div class="content-wrapper">
    <div class="sphinxsidebar">
    <div class="sphinxsidebarwrapper">
        <div class="rel">
    

  <!-- rellinks[1:] is an ugly hack to avoid link to module
  index -->
        <div class="rellink">
        <a href="../model_selection.html"
        accesskey="P">Previous
        <br/>
        <span class="smallrellink">
        3. Model selecti...
        </span>
            <span class="hiddenrellink">
            3. Model selection and evaluation
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="grid_search.html"
        accesskey="N">Next
        <br/>
        <span class="smallrellink">
        3.2. Grid Search...
        </span>
            <span class="hiddenrellink">
            3.2. Grid Search: Searching for estimator parameters
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="../np-modindex.html"
        >Modules
        <br/>
        <span class="smallrellink">
        Python Module In...
        </span>
            <span class="hiddenrellink">
            Python Module Index
            </span>
        </a>
        </div>
            <div class="spacer">
            &nbsp;
            </div>
        <div class="rellink">
        <a href="../py-modindex.html"
        >Modules
        <br/>
        <span class="smallrellink">
        Python Module In...
        </span>
            <span class="hiddenrellink">
            Python Module Index
            </span>
        </a>
        </div>

    <!-- Ad a link to the 'up' page -->
        <div class="spacer">
        &nbsp;
        </div>
        <div class="rellink">
        <a href="../model_selection.html">
        Up
        <br/>
        <span class="smallrellink">
        3. Model selecti...
        </span>
            <span class="hiddenrellink">
            3. Model selection and evaluation
            </span>
            
        </a>
        </div>
    </div>
    
      <p class="doc-version">This documentation is for scikit-learn <strong>version 0.14.1</strong> &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    <p class="citing">If you use the software, please consider <a href="../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <ul>
<li><a class="reference internal" href="#">3.1. Cross-validation: evaluating estimator performance</a><ul>
<li><a class="reference internal" href="#computing-cross-validated-metrics">3.1.1. Computing cross-validated metrics</a></li>
<li><a class="reference internal" href="#cross-validation-iterators">3.1.2. Cross validation iterators</a><ul>
<li><a class="reference internal" href="#k-fold">3.1.2.1. K-fold</a></li>
<li><a class="reference internal" href="#stratified-k-fold">3.1.2.2. Stratified k-fold</a></li>
<li><a class="reference internal" href="#leave-one-out-loo">3.1.2.3. Leave-One-Out - LOO</a></li>
<li><a class="reference internal" href="#leave-p-out-lpo">3.1.2.4. Leave-P-Out - LPO</a></li>
<li><a class="reference internal" href="#leave-one-label-out-lolo">3.1.2.5. Leave-One-Label-Out - LOLO</a></li>
<li><a class="reference internal" href="#leave-p-label-out">3.1.2.6. Leave-P-Label-Out</a></li>
<li><a class="reference internal" href="#random-permutations-cross-validation-a-k-a-shuffle-split">3.1.2.7. Random permutations cross-validation a.k.a. Shuffle &amp; Split</a></li>
<li><a class="reference internal" href="#see-also">3.1.2.8. See also</a></li>
<li><a class="reference internal" href="#bootstrapping-cross-validation">3.1.2.9. Bootstrapping cross-validation</a></li>
</ul>
</li>
<li><a class="reference internal" href="#cross-validation-and-model-selection">3.1.3. Cross validation and model selection</a></li>
</ul>
</li>
</ul>

    </div>
</div>



      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="cross-validation-evaluating-estimator-performance">
<span id="cross-validation"></span><h1>3.1. Cross-validation: evaluating estimator performance<a class="headerlink" href="#cross-validation-evaluating-estimator-performance" title="Permalink to this headline">Â¶</a></h1>
<p>Learning the parameters of a prediction function and testing it on the
same data is a methodological mistake: a model that would just repeat
the labels of the samples that it has just seen would have a perfect
score but would fail to predict anything useful on yet-unseen data.
This situation is called <strong>overfitting</strong>.
To avoid it, it is common practice when performing
a (supervised) machine learning experiment
to hold out part of the available data as a <strong>test set</strong> <tt class="docutils literal"><span class="pre">X_test,</span> <span class="pre">y_test</span></tt>.
Note that the word &#8220;experiment&#8221; is not intended
to denote academic use only,
because even in commercial settings
machine learning usually starts out experimentally.</p>
<p>In scikit-learn a random split into training and test sets
can be quickly computed with the <a class="reference internal" href="generated/sklearn.cross_validation.train_test_split.html#sklearn.cross_validation.train_test_split" title="sklearn.cross_validation.train_test_split"><tt class="xref py py-func docutils literal"><span class="pre">train_test_split</span></tt></a> helper function.
Let&#8217;s load the iris data set to fit a linear support vector machine on it:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">cross_validation</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">datasets</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">svm</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">load_iris</span><span class="p">()</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="o">.</span><span class="n">shape</span>
<span class="go">((150, 4), (150,))</span>
</pre></div>
</div>
<p>We can now quickly sample a training set while holding out 40% of the
data for testing (evaluating) our classifier:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">train_test_split</span><span class="p">(</span>
<span class="gp">... </span>    <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.4</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_train</span><span class="o">.</span><span class="n">shape</span>
<span class="go">((90, 4), (90,))</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_test</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">y_test</span><span class="o">.</span><span class="n">shape</span>
<span class="go">((60, 4), (60,))</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span><span class="o">.</span><span class="n">score</span><span class="p">(</span><span class="n">X_test</span><span class="p">,</span> <span class="n">y_test</span><span class="p">)</span>                           
<span class="go">0.96...</span>
</pre></div>
</div>
<p>When evaluating different settings (&#8220;hyperparameters&#8221;) for estimators,
such as the <tt class="docutils literal"><span class="pre">C</span></tt> setting that must be manually set for an SVM,
there is still a risk of overfitting <em>on the test set</em>
because the parameters can be tweaked until the estimator performs optimally.
This way, knowledge about the test set can &#8220;leak&#8221; into the model
and evaluation metrics no longer report on generalization performance.
To solve this problem, yet another part of the dataset can be held out
as a so-called &#8220;validation set&#8221;: training proceeds on the training set,
after which evaluation is done on the validation set,
and when the experiment seems to be successful,
final evaluation can be done on the test set.</p>
<p>However, by partitioning the available data into three sets,
we drastically reduce the number of samples
which can be used for learning the model,
and the results can depend on a particular random choice for the pair of
(train, validation) sets.</p>
<p>A solution to this problem is a procedure called
<a class="reference external" href="http://en.wikipedia.org/wiki/Cross-validation_(statistics)">cross-validation</a>
(CV for short).
A test set should still be held out for final evaluation,
but the validation set is no longer needed when doing CV.
In the basic approach, called <em>k</em>-fold CV,
the training set is split into <em>k</em> smaller sets
(other approaches are described below,
but generally follow the same principles).
The following procedure is followed for each of the <em>k</em> &#8220;folds&#8221;:</p>
<blockquote>
<div><ul class="simple">
<li>A model is trained using <img class="math" src="../_images/math/aff99fd5805e4be25fcb57c9a2667f8337206a66.png" alt="k-1"/> of the folds as training data;</li>
<li>the resulting model is validated on the remaining part of the data
(i.e., it is used as a test set to compute a performance measure
such as accuracy).</li>
</ul>
</div></blockquote>
<p>The performance measure reported by <em>k</em>-fold cross-validation
is then the average of the values computed in the loop.
This approach can be computationally expensive,
but does not waste too much data
(as it is the case when fixing an arbitrary test set),
which is a major advantage in problem such as inverse inference
where the number of samples is very small.</p>
<div class="section" id="computing-cross-validated-metrics">
<h2>3.1.1. Computing cross-validated metrics<a class="headerlink" href="#computing-cross-validated-metrics" title="Permalink to this headline">Â¶</a></h2>
<p>The simplest way to use cross-validation is to call the
<a class="reference internal" href="generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score" title="sklearn.cross_validation.cross_val_score"><tt class="xref py py-func docutils literal"><span class="pre">cross_val_score</span></tt></a> helper function on the estimator and the dataset.</p>
<p>The following example demonstrates how to estimate the accuracy of a linear
kernel support vector machine on the iris dataset by splitting the data, fitting
a model and computing the score 5 consecutive times (with different splits each
time):</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">clf</span> <span class="o">=</span> <span class="n">svm</span><span class="o">.</span><span class="n">SVC</span><span class="p">(</span><span class="n">kernel</span><span class="o">=</span><span class="s">&#39;linear&#39;</span><span class="p">,</span> <span class="n">C</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span>
<span class="gp">... </span>   <span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
<span class="gp">...</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span>                                              
<span class="go">array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])</span>
</pre></div>
</div>
<p>The mean score and the standard deviation of the score estimate are hence given
by:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="k">print</span><span class="p">(</span><span class="s">&quot;Accuracy: </span><span class="si">%0.2f</span><span class="s"> (+/- </span><span class="si">%0.2f</span><span class="s">)&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">scores</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">scores</span><span class="o">.</span><span class="n">std</span><span class="p">()</span> <span class="o">*</span> <span class="mi">2</span><span class="p">))</span>
<span class="go">Accuracy: 0.98 (+/- 0.03)</span>
</pre></div>
</div>
<p>By default, the score computed at each CV iteration is the <tt class="docutils literal"><span class="pre">score</span></tt>
method of the estimator. It is possible to change this by using the
scoring parameter:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn</span> <span class="kn">import</span> <span class="n">metrics</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">cv</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">scoring</span><span class="o">=</span><span class="s">&#39;f1&#39;</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">scores</span>                                              
<span class="go">array([ 0.96...,  1.  ...,  0.96...,  0.96...,  1.        ])</span>
</pre></div>
</div>
<p>See <a class="reference internal" href="model_evaluation.html#scoring-parameter"><em>The scoring parameter: defining model evaluation rules</em></a> for details.
In the case of the Iris dataset, the samples are balanced across target
classes hence the accuracy and the F1-score are almost equal.</p>
<p>When the <tt class="docutils literal"><span class="pre">cv</span></tt> argument is an integer, <a class="reference internal" href="generated/sklearn.cross_validation.cross_val_score.html#sklearn.cross_validation.cross_val_score" title="sklearn.cross_validation.cross_val_score"><tt class="xref py py-func docutils literal"><span class="pre">cross_val_score</span></tt></a> uses the
<a class="reference internal" href="generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold" title="sklearn.cross_validation.KFold"><tt class="xref py py-class docutils literal"><span class="pre">KFold</span></tt></a> or <a class="reference internal" href="generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold" title="sklearn.cross_validation.StratifiedKFold"><tt class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></tt></a> strategies by default (depending on
the absence or presence of the target array).</p>
<p>It is also possible to use other cross validation strategies by passing a cross
validation iterator instead, for instance:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">n_samples</span> <span class="o">=</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">cv</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">ShuffleSplit</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">test_size</span><span class="o">=</span><span class="mf">0.3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">cross_validation</span><span class="o">.</span><span class="n">cross_val_score</span><span class="p">(</span><span class="n">clf</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="n">iris</span><span class="o">.</span><span class="n">target</span><span class="p">,</span> <span class="n">cv</span><span class="o">=</span><span class="n">cv</span><span class="p">)</span>
<span class="gp">... </span>                                                    
<span class="go">array([ 0.97...,  0.97...,  1.        ])</span>
</pre></div>
</div>
<p>The available cross validation iterators are introduced in the following.</p>
<div class="topic">
<p class="topic-title first">Examples</p>
<ul class="simple">
<li><a class="reference internal" href="../auto_examples/plot_roc_crossval.html#example-plot-roc-crossval-py"><em>Receiver Operating Characteristic (ROC) with cross validation</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/plot_rfe_with_cross_validation.html#example-plot-rfe-with-cross-validation-py"><em>Recursive feature elimination with cross-validation</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/grid_search_digits.html#example-grid-search-digits-py"><em>Parameter estimation using grid search with a nested cross-validation</em></a>,</li>
<li><a class="reference internal" href="../auto_examples/grid_search_text_feature_extraction.html#example-grid-search-text-feature-extraction-py"><em>Sample pipeline for text feature extraction and evaluation</em></a>,</li>
</ul>
</div>
</div>
<div class="section" id="cross-validation-iterators">
<h2>3.1.2. Cross validation iterators<a class="headerlink" href="#cross-validation-iterators" title="Permalink to this headline">Â¶</a></h2>
<p>The following sections list utilities to generate indices
that can be used to generate dataset splits according to different cross
validation strategies.</p>
<div class="section" id="k-fold">
<h3>3.1.2.1. K-fold<a class="headerlink" href="#k-fold" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold" title="sklearn.cross_validation.KFold"><tt class="xref py py-class docutils literal"><span class="pre">KFold</span></tt></a> divides all the samples in <img class="math" src="../_images/math/e9203da50e1059455123460d4e716c9c7f440cc3.png" alt="k"/> groups of samples,
called folds (if <img class="math" src="../_images/math/48b80c2fee19e41a529e3220199a5562ea29e35d.png" alt="k = n"/>, this is equivalent to the <em>Leave One
Out</em> strategy), of equal sizes (if possible). The prediction function is
learned using <img class="math" src="../_images/math/23ca2c534b3fe41683cbc51a0948ba22563356d1.png" alt="k - 1"/> folds, and the fold left out is used for test.</p>
<p>Example of 2-fold cross-validation on a dataset with 4 samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">KFold</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">kf</span> <span class="o">=</span> <span class="n">KFold</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">n_folds</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">kf</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[2 3] [0 1]</span>
<span class="go">[0 1] [2 3]</span>
</pre></div>
</div>
<p>Each fold is constituted by two arrays: the first one is related to the
<em>training set</em>, and the second one to the <em>test set</em>.
Thus, one can create the training/test sets using numpy indexing:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([[</span><span class="mf">0.</span><span class="p">,</span> <span class="mf">0.</span><span class="p">],</span> <span class="p">[</span><span class="mf">1.</span><span class="p">,</span> <span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="o">-</span><span class="mf">1.</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.</span><span class="p">],</span> <span class="p">[</span><span class="mf">2.</span><span class="p">,</span> <span class="mf">2.</span><span class="p">]])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">X</span><span class="p">[</span><span class="n">test</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">train</span><span class="p">],</span> <span class="n">y</span><span class="p">[</span><span class="n">test</span><span class="p">]</span>
</pre></div>
</div>
</div>
<div class="section" id="stratified-k-fold">
<h3>3.1.2.2. Stratified k-fold<a class="headerlink" href="#stratified-k-fold" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold" title="sklearn.cross_validation.StratifiedKFold"><tt class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></tt></a> is a variation of <em>k-fold</em> which returns <em>stratified</em>
folds: each set contains approximately the same percentage of samples of each
target class as the complete set.</p>
<p>Example of stratified 2-fold cross-validation on a dataset with 10 samples from
two slightly unbalanced classes:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">StratifiedKFold</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">skf</span> <span class="o">=</span> <span class="n">StratifiedKFold</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">skf</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[2 3 6 7 8 9] [0 1 4 5]</span>
<span class="go">[0 1 3 4 5 8 9] [2 6 7]</span>
<span class="go">[0 1 2 4 5 6 7] [3 8 9]</span>
</pre></div>
</div>
</div>
<div class="section" id="leave-one-out-loo">
<h3>3.1.2.3. Leave-One-Out - LOO<a class="headerlink" href="#leave-one-out-loo" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.LeaveOneOut.html#sklearn.cross_validation.LeaveOneOut" title="sklearn.cross_validation.LeaveOneOut"><tt class="xref py py-class docutils literal"><span class="pre">LeaveOneOut</span></tt></a> (or LOO) is a simple cross-validation. Each learning
set is created by taking all the samples except one, the test set being
the sample left out. Thus, for <cite>n</cite> samples, we have <cite>n</cite> different learning
sets and <cite>n</cite> different tests set. This cross-validation procedure does
not waste much data as only one sample is removed from the learning set:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeaveOneOut</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">loo</span> <span class="o">=</span> <span class="n">LeaveOneOut</span><span class="p">(</span><span class="mi">4</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">loo</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[1 2 3] [0]</span>
<span class="go">[0 2 3] [1]</span>
<span class="go">[0 1 3] [2]</span>
<span class="go">[0 1 2] [3]</span>
</pre></div>
</div>
<p>Potential users of LOO for model selection should weigh a few known caveats.
When compared with <em>k</em>-fold cross validation, one builds <em>n</em> models from <em>n</em>
samples instead of <em>k</em> models, where <em>n &gt; k</em>. Moreover, each is trained on <em>n - 1</em>
samples rather than <em>(k-1)n / k</em>. In both ways, assuming <em>k</em> is not too large
and <em>k &lt; n</em>, LOO is more computationally expensive than <em>k</em>-fold cross validation.</p>
<p>In terms of accuracy, LOO often results in high variance as an estimator for the
test error. Intuitively, since <em>n - 1</em> of
the <em>n</em> samples are used to build each model, models constructed from folds are
virtually identical to each other and to the model built from the entire training
set.</p>
<p>However, if the learning curve is steep for the training size in question,
then 5- or 10- fold cross validation can overestimate the generalization error.</p>
<p>As a general rule, most authors, and empirical evidence, suggest that 5- or 10-
fold cross validation should be preferred to LOO.</p>
<div class="topic">
<p class="topic-title first">References:</p>
<ul class="simple">
<li><a class="reference external" href="http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html">http://www.faqs.org/faqs/ai-faq/neural-nets/part3/section-12.html</a></li>
<li>T. Hastie, R. Tibshirani, J. Friedman,  <a class="reference external" href="http://www-stat.stanford.edu/~tibs/ElemStatLearn">The Elements of Statistical Learning</a>, Springer 2009</li>
<li>L. Breiman, P. Spector <a class="reference external" href="http://digitalassets.lib.berkeley.edu/sdtr/ucb/text/197.pdf">Submodel selection and evaluation in regression: The X-random case</a>, International Statistical Review 1992</li>
<li>R. Kohavi, <a class="reference external" href="http://www.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf">A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection</a>, Intl. Jnt. Conf. AI</li>
<li>R. Bharat Rao, G. Fung, R. Rosales, <a class="reference external" href="http://www.siam.org/proceedings/datamining/2008/dm08_54_Rao.pdf">On the Dangers of Cross-Validation. An Experimental Evaluation</a>, SIAM 2008</li>
<li>G. James, D. Witten, T. Hastie, R Tibshirani, <a class="reference external" href="http://www-bcf.usc.edu/~gareth/ISL">An Introduction to Statitical Learning</a>, Springer 2013</li>
</ul>
</div>
</div>
<div class="section" id="leave-p-out-lpo">
<h3>3.1.2.4. Leave-P-Out - LPO<a class="headerlink" href="#leave-p-out-lpo" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.LeavePOut.html#sklearn.cross_validation.LeavePOut" title="sklearn.cross_validation.LeavePOut"><tt class="xref py py-class docutils literal"><span class="pre">LeavePOut</span></tt></a> is very similar to <a class="reference internal" href="generated/sklearn.cross_validation.LeaveOneOut.html#sklearn.cross_validation.LeaveOneOut" title="sklearn.cross_validation.LeaveOneOut"><tt class="xref py py-class docutils literal"><span class="pre">LeaveOneOut</span></tt></a> as it creates all
the possible training/test sets by removing <img class="math" src="../_images/math/3eca8557203e86160952e1c0f735f7417f3285b1.png" alt="p"/> samples from the complete
set. For <img class="math" src="../_images/math/413f8a8e40062a9090d9d50b88bc7b551b314c26.png" alt="n"/> samples, this produces <img class="math" src="../_images/math/d72a9aef7b3237cc3733eef2e7aef97a156230e8.png" alt="{n \choose p}"/> train-test
pairs. Unlike <a class="reference internal" href="generated/sklearn.cross_validation.LeaveOneOut.html#sklearn.cross_validation.LeaveOneOut" title="sklearn.cross_validation.LeaveOneOut"><tt class="xref py py-class docutils literal"><span class="pre">LeaveOneOut</span></tt></a> and <a class="reference internal" href="generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold" title="sklearn.cross_validation.KFold"><tt class="xref py py-class docutils literal"><span class="pre">KFold</span></tt></a>, the test sets will
overlap for <img class="math" src="../_images/math/db30f65159c0f4b2e3ec318ed09839aa42bf261a.png" alt="p &gt; 1"/>.</p>
<p>Example of Leave-2-Out on a dataset with 4 samples:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeavePOut</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">lpo</span> <span class="o">=</span> <span class="n">LeavePOut</span><span class="p">(</span><span class="mi">4</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">lpo</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[2 3] [0 1]</span>
<span class="go">[1 3] [0 2]</span>
<span class="go">[1 2] [0 3]</span>
<span class="go">[0 3] [1 2]</span>
<span class="go">[0 2] [1 3]</span>
<span class="go">[0 1] [2 3]</span>
</pre></div>
</div>
</div>
<div class="section" id="leave-one-label-out-lolo">
<h3>3.1.2.5. Leave-One-Label-Out - LOLO<a class="headerlink" href="#leave-one-label-out-lolo" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.LeaveOneLabelOut.html#sklearn.cross_validation.LeaveOneLabelOut" title="sklearn.cross_validation.LeaveOneLabelOut"><tt class="xref py py-class docutils literal"><span class="pre">LeaveOneLabelOut</span></tt></a> (LOLO) is a cross-validation scheme which holds out
the samples according to a third-party provided array of integer labels. This
label information can be used to encode arbitrary domain specific pre-defined
cross-validation folds.</p>
<p>Each training set is thus constituted by all the samples except the ones
related to a specific label.</p>
<p>For example, in the cases of multiple experiments, <em>LOLO</em> can be used to
create a cross-validation based on the different experiments: we create
a training set using the samples of all the experiments except one:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeaveOneLabelOut</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lolo</span> <span class="o">=</span> <span class="n">LeaveOneLabelOut</span><span class="p">(</span><span class="n">labels</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">lolo</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[2 3] [0 1]</span>
<span class="go">[0 1] [2 3]</span>
</pre></div>
</div>
<p>Another common application is to use time information: for instance the
labels could be the year of collection of the samples and thus allow
for cross-validation against time-based splits.</p>
<div class="admonition warning">
<p class="first admonition-title">Warning</p>
<p class="last">Contrary to <a class="reference internal" href="generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold" title="sklearn.cross_validation.StratifiedKFold"><tt class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></tt></a>, <strong>the `labels` of
:class:`LeaveOneLabelOut` should not encode the target class to predict</strong>:
the goal of <a class="reference internal" href="generated/sklearn.cross_validation.StratifiedKFold.html#sklearn.cross_validation.StratifiedKFold" title="sklearn.cross_validation.StratifiedKFold"><tt class="xref py py-class docutils literal"><span class="pre">StratifiedKFold</span></tt></a> is to rebalance dataset classes across
the train / test split to ensure that the train and test folds have
approximately the same percentage of samples of each class while
<a class="reference internal" href="generated/sklearn.cross_validation.LeaveOneLabelOut.html#sklearn.cross_validation.LeaveOneLabelOut" title="sklearn.cross_validation.LeaveOneLabelOut"><tt class="xref py py-class docutils literal"><span class="pre">LeaveOneLabelOut</span></tt></a> will do the opposite by ensuring that the samples
of the train and test fold will not share the same label value.</p>
</div>
</div>
<div class="section" id="leave-p-label-out">
<h3>3.1.2.6. Leave-P-Label-Out<a class="headerlink" href="#leave-p-label-out" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.LeavePLabelOut.html#sklearn.cross_validation.LeavePLabelOut" title="sklearn.cross_validation.LeavePLabelOut"><tt class="xref py py-class docutils literal"><span class="pre">LeavePLabelOut</span></tt></a> is similar as <em>Leave-One-Label-Out</em>, but removes
samples related to <img class="math" src="../_images/math/f48b617185733d8dd6712643f1ab17c736661a06.png" alt="P"/> labels for each training/test set.</p>
<p>Example of Leave-2-Label Out:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="kn">from</span> <span class="nn">sklearn.cross_validation</span> <span class="kn">import</span> <span class="n">LeavePLabelOut</span>

<span class="gp">&gt;&gt;&gt; </span><span class="n">labels</span> <span class="o">=</span> <span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">]</span>
<span class="gp">&gt;&gt;&gt; </span><span class="n">lplo</span> <span class="o">=</span> <span class="n">LeavePLabelOut</span><span class="p">(</span><span class="n">labels</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="ow">in</span> <span class="n">lplo</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">test</span><span class="p">))</span>
<span class="go">[4 5] [0 1 2 3]</span>
<span class="go">[2 3] [0 1 4 5]</span>
<span class="go">[0 1] [2 3 4 5]</span>
</pre></div>
</div>
</div>
<div class="section" id="random-permutations-cross-validation-a-k-a-shuffle-split">
<span id="shufflesplit"></span><h3>3.1.2.7. Random permutations cross-validation a.k.a. Shuffle &amp; Split<a class="headerlink" href="#random-permutations-cross-validation-a-k-a-shuffle-split" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit" title="sklearn.cross_validation.ShuffleSplit"><tt class="xref py py-class docutils literal"><span class="pre">ShuffleSplit</span></tt></a></p>
<p>The <a class="reference internal" href="generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit" title="sklearn.cross_validation.ShuffleSplit"><tt class="xref py py-class docutils literal"><span class="pre">ShuffleSplit</span></tt></a> iterator will generate a user defined number of
independent train / test dataset splits. Samples are first shuffled and
then split into a pair of train and test sets.</p>
<p>It is possible to control the randomness for reproducibility of the
results by explicitly seeding the <tt class="docutils literal"><span class="pre">random_state</span></tt> pseudo random number
generator.</p>
<p>Here is a usage example:</p>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">ss</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">ShuffleSplit</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.25</span><span class="p">,</span>
<span class="gp">... </span>    <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">ss</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span><span class="p">))</span>
<span class="gp">...</span>
<span class="go">[1 3 4] [2 0]</span>
<span class="go">[1 4 3] [0 2]</span>
<span class="go">[4 0 2] [1 3]</span>
</pre></div>
</div>
<p><a class="reference internal" href="generated/sklearn.cross_validation.ShuffleSplit.html#sklearn.cross_validation.ShuffleSplit" title="sklearn.cross_validation.ShuffleSplit"><tt class="xref py py-class docutils literal"><span class="pre">ShuffleSplit</span></tt></a> is thus a good alternative to <a class="reference internal" href="generated/sklearn.cross_validation.KFold.html#sklearn.cross_validation.KFold" title="sklearn.cross_validation.KFold"><tt class="xref py py-class docutils literal"><span class="pre">KFold</span></tt></a> cross
validation that allows a finer control on the number of iterations and
the proportion of samples in on each side of the train / test split.</p>
</div>
<div class="section" id="see-also">
<h3>3.1.2.8. See also<a class="headerlink" href="#see-also" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.StratifiedShuffleSplit.html#sklearn.cross_validation.StratifiedShuffleSplit" title="sklearn.cross_validation.StratifiedShuffleSplit"><tt class="xref py py-class docutils literal"><span class="pre">StratifiedShuffleSplit</span></tt></a> is a variation of <em>ShuffleSplit</em>, which returns
stratified splits, <em>i.e</em> which creates splits by preserving the same
percentage for each target class as in the complete set.</p>
</div>
<div class="section" id="bootstrapping-cross-validation">
<span id="bootstrap"></span><h3>3.1.2.9. Bootstrapping cross-validation<a class="headerlink" href="#bootstrapping-cross-validation" title="Permalink to this headline">Â¶</a></h3>
<p><a class="reference internal" href="generated/sklearn.cross_validation.Bootstrap.html#sklearn.cross_validation.Bootstrap" title="sklearn.cross_validation.Bootstrap"><tt class="xref py py-class docutils literal"><span class="pre">Bootstrap</span></tt></a></p>
<p><a class="reference external" href="http://en.wikipedia.org/wiki/Bootstrapping_%28statistics%29">Bootstrapping</a> is a general statistics technique that iterates the
computation of an estimator on a resampled dataset.</p>
<p>The <a class="reference internal" href="generated/sklearn.cross_validation.Bootstrap.html#sklearn.cross_validation.Bootstrap" title="sklearn.cross_validation.Bootstrap"><tt class="xref py py-class docutils literal"><span class="pre">Bootstrap</span></tt></a> iterator will generate a user defined number
of independent train / test dataset splits. Samples are then drawn
(with replacement) on each side of the split. It furthermore possible
to control the size of the train and test subset to make their union
smaller than the total dataset if it is very large.</p>
<div class="admonition note">
<p class="first admonition-title">Note</p>
<p class="last">Contrary to other cross-validation strategies, bootstrapping
will allow some samples to occur several times in each splits.</p>
</div>
<div class="highlight-python"><div class="highlight"><pre><span class="gp">&gt;&gt;&gt; </span><span class="n">bs</span> <span class="o">=</span> <span class="n">cross_validation</span><span class="o">.</span><span class="n">Bootstrap</span><span class="p">(</span><span class="mi">9</span><span class="p">,</span> <span class="n">n_iter</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span><span class="k">for</span> <span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span> <span class="ow">in</span> <span class="n">bs</span><span class="p">:</span>
<span class="gp">... </span>    <span class="k">print</span><span class="p">(</span><span class="s">&quot;</span><span class="si">%s</span><span class="s"> </span><span class="si">%s</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">train_index</span><span class="p">,</span> <span class="n">test_index</span><span class="p">))</span>
<span class="gp">...</span>
<span class="go">[1 8 7 7 8] [0 3 0 5]</span>
<span class="go">[5 4 2 4 2] [6 7 1 0]</span>
<span class="go">[4 7 0 1 1] [5 3 6 5]</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="cross-validation-and-model-selection">
<h2>3.1.3. Cross validation and model selection<a class="headerlink" href="#cross-validation-and-model-selection" title="Permalink to this headline">Â¶</a></h2>
<p>Cross validation iterators can also be used to directly perform model
selection using Grid Search for the optimal hyperparameters of the
model. This is the topic if the next section: <a class="reference internal" href="grid_search.html#grid-search"><em>Grid Search: Searching for estimator parameters</em></a>.</p>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010 - 2014, scikit-learn developers (BSD License).
      <a href="../_sources/modules/cross_validation.txt" rel="nofollow">Show this page source</a>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="../model_selection.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="grid_search.html">Next
      </a>
    </div>
    <div class="buttonPrevious">
      <a href="../np-modindex.html">Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="../py-modindex.html">Next
      </a>
    </div>
    
     </div>

    
    <script type="text/javascript">
      var _gaq = _gaq || [];
      _gaq.push(['_setAccount', 'UA-22606712-2']);
      _gaq.push(['_trackPageview']);

      (function() {
        var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
        ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
        var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
      })();
    </script>
    

    <script src="http://www.google.com/jsapi" type="text/javascript"></script>
    <script type="text/javascript"> google.load('search', '1',
        {language : 'en'}); google.setOnLoadCallback(function() {
            var customSearchControl = new
            google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
            customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
            var options = new google.search.DrawOptions();
            options.setAutoComplete(true);
            customSearchControl.draw('cse', options); }, true);
    </script>
  </body>
</html>