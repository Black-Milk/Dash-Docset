

<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN"
  "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">


<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
    
    <title>Sparse recovery: feature selection for sparse linear models &mdash; scikit-learn 0.13.1 documentation</title>
    
    <link rel="stylesheet" href="../../_static/nature.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    
    <script type="text/javascript">
      var DOCUMENTATION_OPTIONS = {
        URL_ROOT:    '../../',
        VERSION:     '0.13.1',
        COLLAPSE_INDEX: false,
        FILE_SUFFIX: '.html',
        HAS_SOURCE:  true
      };
    </script>
    <script type="text/javascript" src="../../_static/jquery.js"></script>
    <script type="text/javascript" src="../../_static/underscore.js"></script>
    <script type="text/javascript" src="../../_static/doctools.js"></script>
    <script type="text/javascript" src="../../_static/sidebar.js"></script>
    <link rel="shortcut icon" href="../../_static/favicon.ico"/>
    <link rel="author" title="About these documents" href="../../about.html" />
    <link rel="top" title="scikit-learn 0.13.1 documentation" href="../../index.html" />
    <link rel="up" title="Examples" href="../index.html" />
    <link rel="next" title="Swiss Roll reduction with LLE" href="../manifold/plot_swissroll.html" />
    <link rel="prev" title="Lasso model selection: Cross-Validation / AIC / BIC" href="plot_lasso_model_selection.html" />


<script type="text/javascript">

  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-22606712-2']);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();

</script>



  </head>
  <body>

    <div class="header-wrapper">
      <div class="header">
          <p class="logo"><a href="../../index.html">
            <img src="../../_static/scikit-learn-logo-small.png" alt="Logo"/>
          </a>
          </p><div class="navbar">
          <ul>
            <li><a href="../../install.html">Download</a></li>
            <li><a href="../../support.html">Support</a></li>
            <li><a href="../../user_guide.html">User Guide</a></li>
            <li><a href="../index.html">Examples</a></li>
            <li><a href="../../modules/classes.html">Reference</a></li>
       </ul>

<div class="search_form">

<div id="cse" style="width: 100%;"></div>
<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">
  google.load('search', '1', {language : 'en'});
  google.setOnLoadCallback(function() {
    var customSearchControl = new google.search.CustomSearchControl('016639176250731907682:tjtqbvtvij0');
    customSearchControl.setResultSetSize(google.search.Search.FILTERED_CSE_RESULTSET);
    var options = new google.search.DrawOptions();
    options.setAutoComplete(true);
    customSearchControl.draw('cse', options);
  }, true);
</script>

</div>
          </div> <!-- end navbar --></div>
    </div>

    <div class="content-wrapper">

      <div class="sphinxsidebar">
	<div class="sphinxsidebarwrapper">
	  <div class="rel">
	   
	<!-- rellinks[1:] is an ugly hack to avoid link to module
	    index  -->
	<div class="rellink">
	<a href="plot_lasso_model_selection.html" title="Lasso model selection: Cross-Validation / AIC / BIC"
	    accesskey="P">Previous
	    <br>
	    <span class="smallrellink">
	    Lasso model sele...
	    </span>
	    <span class="hiddenrellink">
	    Lasso model selection: Cross-Validation / AIC / BIC
	    </span>
	    
	    </a>
	</div>
	    <div class="spacer">
	    &nbsp;
	    </div>
	
	<div class="rellink">
	<a href="../manifold/plot_swissroll.html" title="Swiss Roll reduction with LLE"
	    accesskey="N">Next
	    <br>
	    <span class="smallrellink">
	    Swiss Roll reduc...
	    </span>
	    <span class="hiddenrellink">
	    Swiss Roll reduction with LLE
	    </span>
	    
	    </a>
	</div>
	<!-- Ad a link to the 'up' page -->
	<div class="spacer">
	&nbsp;
	</div>
	<div class="rellink">
	<a href="../index.html" title="Examples" >
	Up
	<br>
	<span class="smallrellink">
	Examples
	</span>
	<span class="hiddenrellink">
	Examples
	</span>
	
	</a>
	</div>
    </div>
    <p style="text-align: center; background-color: #FFE4E4">This documentation is
    for scikit-learn <strong>version 0.13.1</strong>
    &mdash; <a href="http://scikit-learn.org/stable/support.html#documentation-resources">Other versions</a></p>
    
    <h3><a href="../../about.html#citing-scikit-learn">Citing</a></h3>
    <p>If you use the software, please consider
    <a href="../../about.html#citing-scikit-learn">citing scikit-learn</a>.</p>
    <h3>This page</h3>
	<ul>
<li><a class="reference internal" href="#">Sparse recovery: feature selection for sparse linear models</a></li>
</ul>

    
    </div>
	  </div>


      <div class="content">
            
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body">
            
  <div class="section" id="sparse-recovery-feature-selection-for-sparse-linear-models">
<span id="example-linear-model-plot-sparse-recovery-py"></span><h1>Sparse recovery: feature selection for sparse linear models<a class="headerlink" href="#sparse-recovery-feature-selection-for-sparse-linear-models" title="Permalink to this headline">Â¶</a></h1>
<p>Given a small number of observations, we want to recover which features
of X are relevant to explain y. For this <a class="reference internal" href="../../modules/feature_selection.html#l1-feature-selection"><em>sparse linear models</em></a> can outperform standard statistical tests if the
true model is sparse, i.e. if a small fraction of the features are
relevant.</p>
<p>As detailed in <a class="reference internal" href="../../modules/feature_selection.html#compressive-sensing"><em>the compressive sensing notes</em></a>, the ability of L1-based approach to identify the
relevant variables depends on the sparsity of the ground truth, the
number of samples, the number of features, the conditionning of the
design matrix on the signal subspace, the amount of noise, and the
absolute value of the smallest non-zero coefficient [Wainwright2006]
(<a class="reference external" href="http://statistics.berkeley.edu/tech-reports/709.pdf">http://statistics.berkeley.edu/tech-reports/709.pdf</a>).</p>
<p>Here we keep all parameters constant and vary the conditionning of the
design matrix. For a well-conditionned design matrix (small mutual
incoherence) we are exactly in compressive sensing conditions (i.i.d
Gaussian sensing matrix), and L1-recovery with the Lasso performs very
well. For an ill-conditionned matrix (high mutual incoherence),
regressors are very correlated, and the Lasso randomly selects one.
However, randomized-Lasso can recover the ground truth well.</p>
<p>In each situation, we first vary the alpha parameter setting the sparsity
of the estimated model and look at the stability scores of the randomized
Lasso. This analysis, knowing the ground truth, shows an optimal regime
in which relevant features stand out from the irrelevant ones. If alpha
is chosen too small, non-relevant variables enter the model. On the
opposite, if alpha is selected too large, the Lasso is equivalent to
stepwise regression, and thus brings no advantage over a univariate
F-test.</p>
<p>In a second time, we set alpha and compare the performance of different
feature selection methods, using the area under curve (AUC) of the
precision-recall.</p>
<p><strong>Python source code:</strong> <a class="reference download internal" href="../../_downloads/plot_sparse_recovery.py"><tt class="xref download docutils literal"><span class="pre">plot_sparse_recovery.py</span></tt></a></p>
<div class="highlight-python"><div class="highlight"><pre><span class="k">print</span><span class="p">(</span><span class="n">__doc__</span><span class="p">)</span>

<span class="c"># Author: Alexandre Gramfort and Gael Varoquaux</span>
<span class="c"># License: BSD 3 clause</span>

<span class="kn">import</span> <span class="nn">warnings</span>

<span class="kn">import</span> <span class="nn">pylab</span> <span class="kn">as</span> <span class="nn">pl</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="kn">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">linalg</span>

<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="p">(</span><span class="n">RandomizedLasso</span><span class="p">,</span> <span class="n">lasso_stability_path</span><span class="p">,</span>
                                  <span class="n">LassoLarsCV</span><span class="p">)</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">StandardScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">auc</span><span class="p">,</span> <span class="n">precision_recall_curve</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">ExtraTreesRegressor</span>
<span class="kn">from</span> <span class="nn">sklearn.utils.extmath</span> <span class="kn">import</span> <span class="n">pinvh</span>


<span class="k">def</span> <span class="nf">mutual_incoherence</span><span class="p">(</span><span class="n">X_relevant</span><span class="p">,</span> <span class="n">X_irelevant</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Mutual incoherence, as defined by formula (26a) of [Wainwright2006].</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">projector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_irelevant</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_relevant</span><span class="p">),</span>
                       <span class="n">pinvh</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X_relevant</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">X_relevant</span><span class="p">)))</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">projector</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>


<span class="k">for</span> <span class="n">conditionning</span> <span class="ow">in</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mf">1e-4</span><span class="p">):</span>
    <span class="c">###########################################################################</span>
    <span class="c"># Simulate regression data with a correlated design</span>
    <span class="n">n_features</span> <span class="o">=</span> <span class="mi">501</span>
    <span class="n">n_relevant_features</span> <span class="o">=</span> <span class="mi">3</span>
    <span class="n">noise_level</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
    <span class="n">coef_min</span> <span class="o">=</span> <span class="o">.</span><span class="mi">2</span>
    <span class="c"># The Donoho-Tanner phase transition is around n_samples=25: below we</span>
    <span class="c"># will completely fail to recover in the well-conditionned case</span>
    <span class="n">n_samples</span> <span class="o">=</span> <span class="mi">25</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="n">n_relevant_features</span>

    <span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>

    <span class="c"># The coefficients of our model</span>
    <span class="n">coef</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_features</span><span class="p">)</span>
    <span class="n">coef</span><span class="p">[:</span><span class="n">n_relevant_features</span><span class="p">]</span> <span class="o">=</span> <span class="n">coef_min</span> <span class="o">+</span> <span class="n">rng</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_relevant_features</span><span class="p">)</span>

    <span class="c"># The correlation of our design: variables correlated by blocs of 3</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_features</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">n_features</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="n">corr</span><span class="p">[</span><span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">i</span><span class="p">:</span><span class="n">i</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">conditionning</span>
    <span class="n">corr</span><span class="o">.</span><span class="n">flat</span><span class="p">[::</span><span class="n">n_features</span> <span class="o">+</span> <span class="mi">1</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
    <span class="n">corr</span> <span class="o">=</span> <span class="n">linalg</span><span class="o">.</span><span class="n">cholesky</span><span class="p">(</span><span class="n">corr</span><span class="p">)</span>

    <span class="c"># Our design</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">n_samples</span><span class="p">,</span> <span class="n">n_features</span><span class="p">))</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">corr</span><span class="p">)</span>
    <span class="c"># Keep [Wainwright2006] (26c) constant</span>
    <span class="n">X</span><span class="p">[:</span><span class="n">n_relevant_features</span><span class="p">]</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span>
        <span class="n">linalg</span><span class="o">.</span><span class="n">svdvals</span><span class="p">(</span><span class="n">X</span><span class="p">[:</span><span class="n">n_relevant_features</span><span class="p">]))</span><span class="o">.</span><span class="n">max</span><span class="p">()</span>
    <span class="n">X</span> <span class="o">=</span> <span class="n">StandardScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">copy</span><span class="p">())</span>

    <span class="c"># The output variable</span>
    <span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">coef</span><span class="p">)</span>
    <span class="n">y</span> <span class="o">/=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="c"># We scale the added noise as a function of the average correlation</span>
    <span class="c"># between the design and the output variable</span>
    <span class="n">y</span> <span class="o">+=</span> <span class="n">noise_level</span> <span class="o">*</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="n">n_samples</span><span class="p">)</span>
    <span class="n">mi</span> <span class="o">=</span> <span class="n">mutual_incoherence</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="n">n_relevant_features</span><span class="p">],</span>
                            <span class="n">X</span><span class="p">[:,</span> <span class="n">n_relevant_features</span><span class="p">:])</span>

    <span class="c">###########################################################################</span>
    <span class="c"># Plot stability selection path, using a high eps for early stopping</span>
    <span class="c"># of the path, to save computation time</span>
    <span class="n">alpha_grid</span><span class="p">,</span> <span class="n">scores_path</span> <span class="o">=</span> <span class="n">lasso_stability_path</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">,</span>
                                                   <span class="n">eps</span><span class="o">=</span><span class="mf">0.05</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="c"># We plot the path as a function of alpha/alpha_max to the power 1/3: the</span>
    <span class="c"># power 1/3 scales the path less brutally than the log, and enables to</span>
    <span class="c"># see the progression along the path</span>
    <span class="n">hg</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_grid</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="o">.</span><span class="mi">333</span><span class="p">,</span> <span class="n">scores_path</span><span class="p">[</span><span class="n">coef</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s">&#39;r&#39;</span><span class="p">)</span>
    <span class="n">hb</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">alpha_grid</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span> <span class="o">**</span> <span class="o">.</span><span class="mi">333</span><span class="p">,</span> <span class="n">scores_path</span><span class="p">[</span><span class="n">coef</span> <span class="o">==</span> <span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">T</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="s">&#39;k&#39;</span><span class="p">)</span>
    <span class="n">ymin</span><span class="p">,</span> <span class="n">ymax</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">ylim</span><span class="p">()</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">r&#39;$(\alpha / \alpha_{max})^{1/3}$&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&#39;Stability score: proportion of times selected&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Stability Scores Path - Mutual incoherence: </span><span class="si">%.1f</span><span class="s">&#39;</span> <span class="o">%</span> <span class="n">mi</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">axis</span><span class="p">(</span><span class="s">&#39;tight&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">((</span><span class="n">hg</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hb</span><span class="p">[</span><span class="mi">0</span><span class="p">]),</span> <span class="p">(</span><span class="s">&#39;relevant features&#39;</span><span class="p">,</span> <span class="s">&#39;irrelevant features&#39;</span><span class="p">),</span>
              <span class="n">loc</span><span class="o">=</span><span class="s">&#39;best&#39;</span><span class="p">)</span>

    <span class="c">###########################################################################</span>
    <span class="c"># Plot the estimated stability scores for a given alpha</span>

    <span class="c"># Use 6-fold cross-validation rather than the default 3-fold: it leads to</span>
    <span class="c"># a better choice of alpha:</span>
    <span class="c"># Stop the user warnings outputs- they are not necessary for the example</span>
    <span class="c"># as it is specifically set up to be challenging.</span>
    <span class="k">with</span> <span class="n">warnings</span><span class="o">.</span><span class="n">catch_warnings</span><span class="p">():</span>
        <span class="n">warnings</span><span class="o">.</span><span class="n">simplefilter</span><span class="p">(</span><span class="s">&#39;ignore&#39;</span><span class="p">,</span> <span class="ne">UserWarning</span><span class="p">)</span>
        <span class="n">lars_cv</span> <span class="o">=</span> <span class="n">LassoLarsCV</span><span class="p">(</span><span class="n">cv</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c"># Run the RandomizedLasso: we use a paths going down to .1*alpha_max</span>
    <span class="c"># to avoid exploring the regime in which very noisy variables enter</span>
    <span class="c"># the model</span>
    <span class="n">alphas</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">lars_cv</span><span class="o">.</span><span class="n">alphas_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="o">.</span><span class="mi">1</span> <span class="o">*</span> <span class="n">lars_cv</span><span class="o">.</span><span class="n">alphas_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">6</span><span class="p">)</span>
    <span class="n">clf</span> <span class="o">=</span> <span class="n">RandomizedLasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="n">alphas</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">trees</span> <span class="o">=</span> <span class="n">ExtraTreesRegressor</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="c"># Compare with F-score</span>
    <span class="n">F</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">f_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">figure</span><span class="p">()</span>
    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">score</span> <span class="ow">in</span> <span class="p">[(</span><span class="s">&#39;F-test&#39;</span><span class="p">,</span> <span class="n">F</span><span class="p">),</span>
                        <span class="p">(</span><span class="s">&#39;Stability selection&#39;</span><span class="p">,</span> <span class="n">clf</span><span class="o">.</span><span class="n">scores_</span><span class="p">),</span>
                        <span class="p">(</span><span class="s">&#39;Lasso coefs&#39;</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">lars_cv</span><span class="o">.</span><span class="n">coef_</span><span class="p">)),</span>
                        <span class="p">(</span><span class="s">&#39;Trees&#39;</span><span class="p">,</span> <span class="n">trees</span><span class="o">.</span><span class="n">feature_importances_</span><span class="p">),</span>
                        <span class="p">]:</span>
        <span class="n">precision</span><span class="p">,</span> <span class="n">recall</span><span class="p">,</span> <span class="n">thresholds</span> <span class="o">=</span> <span class="n">precision_recall_curve</span><span class="p">(</span><span class="n">coef</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">,</span>
                                                               <span class="n">score</span><span class="p">)</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">semilogy</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="n">score</span> <span class="o">/</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">score</span><span class="p">),</span> <span class="mf">1e-4</span><span class="p">),</span>
                    <span class="n">label</span><span class="o">=</span><span class="s">&quot;</span><span class="si">%s</span><span class="s">. AUC: </span><span class="si">%.3f</span><span class="s">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">auc</span><span class="p">(</span><span class="n">recall</span><span class="p">,</span> <span class="n">precision</span><span class="p">)))</span>

    <span class="n">pl</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">coef</span> <span class="o">!=</span> <span class="mi">0</span><span class="p">)[</span><span class="mi">0</span><span class="p">],</span> <span class="p">[</span><span class="mf">2e-4</span><span class="p">]</span> <span class="o">*</span> <span class="n">n_relevant_features</span><span class="p">,</span> <span class="s">&#39;mo&#39;</span><span class="p">,</span>
            <span class="n">label</span><span class="o">=</span><span class="s">&quot;Ground truth&quot;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s">&quot;Features&quot;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s">&quot;Score&quot;</span><span class="p">)</span>
    <span class="c"># Plot only the 100 first coefficients</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s">&#39;best&#39;</span><span class="p">)</span>
    <span class="n">pl</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s">&#39;Feature selection scores - Mutual incoherence: </span><span class="si">%.1f</span><span class="s">&#39;</span>
             <span class="o">%</span> <span class="n">mi</span><span class="p">)</span>

<span class="n">pl</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>


          </div>
        </div>
      </div>
        <div class="clearer"></div>
      </div>
    </div>

    <div class="footer">
        &copy; 2010â2013, scikit-learn developers (BSD License).
      Created using <a href="http://sphinx.pocoo.org/">Sphinx</a> 1.1.3. Design by <a href="http://desgrana.es">Desgrana</a>.
    <span style="padding-left: 5ex;">
    <a href="../../_sources/auto_examples/linear_model/plot_sparse_recovery.txt"
	    rel="nofollow">Show this page source</a>
    </span>
    </div>
     <div class="rel">
    
    <div class="buttonPrevious">
      <a href="plot_lasso_model_selection.html">
        Previous
      </a>
    </div>
    <div class="buttonNext">
      <a href="../manifold/plot_swissroll.html">
        Next
      </a>
    </div>
    
     </div>
     <script type="text/javascript">
       $("div.buttonNext, div.buttonPrevious").hover(
           function () {
               $(this).css('background-color', '#FF9C34');
           },
           function () {
               $(this).css('background-color', '#A7D6E2');
           }
       );
       var bodywrapper = $('.bodywrapper');
   	var sidebarbutton = $('#sidebarbutton');
        sidebarbutton.css({
	    'height': '900px'
       });
     </script>
  </body>
</html>